"""
Main Window - Primary application window with speaker-audio mapping
"""
import sys
import os
from pathlib import Path
from typing import Optional

from PyQt6.QtWidgets import (
    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
    QPushButton, QLabel, QFileDialog, QListWidget, QListWidgetItem,
    QSplitter, QTextEdit, QSlider, QSpinBox, QDoubleSpinBox, QProgressBar, QDialog,
    QGroupBox, QMessageBox, QTableWidget, QTableWidgetItem,
    QHeaderView, QComboBox, QToolBar, QStyle, QMenu, QStatusBar, QSizePolicy,
    QStyledItemDelegate, QStyleOptionViewItem, QAbstractItemView,
    QLineEdit, QPlainTextEdit
)
from PyQt6.QtCore import Qt, QThread, pyqtSignal, QSize, QEvent, QRect, QFileSystemWatcher, QTimer
from PyQt6.QtGui import QColor, QIcon, QPixmap, QPalette, QFontMetrics
from typing import TYPE_CHECKING

import copy
from .timeline_widget import TimelineWidget
from .clip import TimelineClip
from .undo_system import UndoStack, ModifyClipsCommand, AddRemoveClipsCommand, ReplaceAllClipsCommand, MacroCommand
from .preview_widget import PreviewWidget
from .settings_widget import SettingsWidget, SettingsDialog
from .render_settings_dialog import RenderSettingsDialog
from .theme import ModernDarkTheme
from .recent_projects import get_recent_projects_manager
from config import DEFAULT_GAP_SECONDS
from runtime_config import get_config, set_config, RuntimeConfig

if TYPE_CHECKING:
    from pydub import AudioSegment

from .file_list_widget import ImageGridDelegate, DraggableImageListWidget
from .progress_dialog import ProgressDialog
from .threads import RenderThread, ProcessingThread

class MainWindow(QMainWindow):
    """Main application window"""
    
    def __init__(self):
        super().__init__()
        self.setWindowTitle("PictureBookBuilder")
        self.setMinimumSize(1200, 800)
        
        # Set window icon
        if getattr(sys, 'frozen', False):
            # PyInstaller mode
            base_path = Path(sys._MEIPASS)
        else:
            # Normal mode
            base_path = Path(__file__).parent.parent.parent
            
        icon_path = base_path / "assets" / "icon.png"
        if icon_path.exists():
            self.setWindowIcon(QIcon(str(icon_path)))
        
        self.script_path: Optional[str] = None
        self.image_folder: Optional[str] = None
        self.speakers: list[str] = []
        self.speaker_audio_map: dict[str, str] = {}
        self.audio_files: list[Path] = []
        self.result_data: Optional[dict] = None
        self.project_path: Optional[str] = None  # Current project file path
        
        # Audio cache for fast waveform extraction during real-time edit
        self.speaker_audio_cache: dict[str, 'AudioSegment'] = {}
        self._waveform_cache: dict[str, list[float]] = {}  # Cache by (clip_id, start, end)
        
        # Runtime configuration
        self.runtime_config = get_config()
        
        # Undo system
        self.undo_stack = UndoStack()

        # State management for non-undoable changes (e.g. script/audio mapping)
        self._manual_modification_flag = False

        # File watcher for image directory
        self.image_watcher = QFileSystemWatcher(self)
        self.image_watcher.directoryChanged.connect(self._on_directory_changed)
        
        # Timer for debouncing file updates
        self.image_update_timer = QTimer(self)
        self.image_update_timer.setSingleShot(True)
        self.image_update_timer.timeout.connect(self._on_image_update_timeout)

        self._setup_menu_bar()
        self._setup_ui()

    def closeEvent(self, event):
        """Handle application close - cleanup resources"""
        if not self._check_unsaved_changes():
            event.ignore()
            return

        # Stop any running background threads to prevent thread leaks
        if hasattr(self, 'processing_thread') and self.processing_thread is not None:
            if self.processing_thread.isRunning():
                self.processing_thread.quit()
                self.processing_thread.wait()
        
        if hasattr(self, 'render_thread') and self.render_thread is not None:
            if self.render_thread.isRunning():
                self.render_thread.quit()
                self.render_thread.wait()
        
        # Cleanup preview widget (includes AudioMixer cleanup)
        if hasattr(self, 'preview_widget'):
            self.preview_widget.cleanup()
        
        # Cleanup global image cache
        from .image_cache import get_image_cache
        get_image_cache().cleanup()
        
        event.accept()
    
    def _make_unique_clip_id(self, base_id: str) -> str:
        """Generate a clip id that is unique within the current timeline."""
        clips = getattr(self.timeline_widget.canvas, 'clips', [])
        used_ids = {getattr(c, 'id', None) for c in clips}
        used_ids.discard(None)

        if base_id not in used_ids:
            return base_id

        suffix = 1
        while f"{base_id}_{suffix}" in used_ids:
            suffix += 1
        return f"{base_id}_{suffix}"
    
    def _setup_menu_bar(self):
        """Setup the menu bar with comprehensive options"""
        from PyQt6.QtGui import QAction, QKeySequence
        
        menu_bar = self.menuBar()
        
        # --- File Menu ---
        file_menu = menu_bar.addMenu("íŒŒì¼")
        
        # New project
        new_action = QAction("ìƒˆ í”„ë¡œì íŠ¸", self)
        new_action.setShortcut(QKeySequence.StandardKey.New)
        new_action.triggered.connect(self._new_project)
        file_menu.addAction(new_action)
        
        # Open project
        open_action = QAction("í”„ë¡œì íŠ¸ ì—´ê¸°...", self)
        open_action.setShortcut(QKeySequence.StandardKey.Open)
        open_action.triggered.connect(self._open_project)
        file_menu.addAction(open_action)
        
        file_menu.addSeparator()
        
        # Save project
        save_action = QAction("ì €ì¥", self)
        save_action.setShortcut(QKeySequence.StandardKey.Save)
        save_action.triggered.connect(self._save_project)
        file_menu.addAction(save_action)
        
        # Save as
        save_as_action = QAction("ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ì €ì¥...", self)
        save_as_action.setShortcut(QKeySequence("Ctrl+Shift+S"))
        save_as_action.triggered.connect(self._save_project_as)
        file_menu.addAction(save_as_action)
        
        file_menu.addSeparator()
        
        # Settings (Moved to File menu)
        settings_action = QAction("ì„¤ì •...", self)
        settings_action.triggered.connect(self._show_settings)
        file_menu.addAction(settings_action)
        
        file_menu.addSeparator()
        
        exit_action = QAction("ì¢…ë£Œ", self)
        exit_action.setShortcut(QKeySequence.StandardKey.Quit)
        exit_action.triggered.connect(self.close)
        file_menu.addAction(exit_action)
        
        # --- Edit Menu ---
        edit_menu = menu_bar.addMenu("í¸ì§‘")

        self.undo_action = QAction("ì‹¤í–‰ ì·¨ì†Œ", self)
        self.undo_action.setShortcut(QKeySequence.StandardKey.Undo)
        self.undo_action.triggered.connect(self._undo)
        self.undo_action.setEnabled(False)
        edit_menu.addAction(self.undo_action)

        self.redo_action = QAction("ë‹¤ì‹œ ì‹¤í–‰", self)
        self.redo_action.setShortcut(QKeySequence.StandardKey.Redo)
        self.redo_action.triggered.connect(self._redo)
        self.redo_action.setEnabled(False)
        edit_menu.addAction(self.redo_action)

        # --- Project Menu ---
        project_menu = menu_bar.addMenu("í”„ë¡œì íŠ¸")
        
        load_script_action = QAction("ìŠ¤í¬ë¦½íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°...", self)
        load_script_action.triggered.connect(self._load_script)
        project_menu.addAction(load_script_action)
        
        load_images_action = QAction("ì´ë¯¸ì§€ í´ë” ë¶ˆëŸ¬ì˜¤ê¸°...", self)
        load_images_action.triggered.connect(self._load_image_folder)
        project_menu.addAction(load_images_action)
        
        self.reload_images_action = QAction("ì´ë¯¸ì§€ í´ë” ë‹¤ì‹œì½ê¸°", self)
        self.reload_images_action.setShortcut("F6")
        self.reload_images_action.triggered.connect(self._reload_image_folder)
        self.reload_images_action.setEnabled(False)  # ì´ë¯¸ì§€ í´ë”ê°€ ì„¤ì •ë˜ë©´ í™œì„±í™”
        project_menu.addAction(self.reload_images_action)
        
        # --- Tools Menu ---
        tools_menu = menu_bar.addMenu("ë„êµ¬")
        
        self.action_process = QAction("ì²˜ë¦¬ ì‹œì‘", self)
        self.action_process.setShortcut(QKeySequence("F5"))
        self.action_process.triggered.connect(self._start_processing)
        self.action_process.setEnabled(False)
        tools_menu.addAction(self.action_process)
        
        tools_menu.addSeparator()
        
        self.action_format_subs = QAction("ìë§‰ ìë™ ì •ë¦¬", self)
        self.action_format_subs.triggered.connect(self._auto_format_subtitles)
        self.action_format_subs.setEnabled(False)
        tools_menu.addAction(self.action_format_subs)
        
        self.action_apply_images = QAction("ì´ë¯¸ì§€ ì¼ê´„ ì ìš©", self)
        self.action_apply_images.triggered.connect(self._apply_images_to_timeline)
        self.action_apply_images.setEnabled(False)
        tools_menu.addAction(self.action_apply_images)
        
        self.action_auteur_import = QAction("Auteurì—ì„œ ì´ë¯¸ì§€ ë°°ì¹˜...", self)
        self.action_auteur_import.triggered.connect(self._import_from_auteur)
        self.action_auteur_import.setEnabled(False)
        tools_menu.addAction(self.action_auteur_import)
        
        # --- Export Menu ---
        export_menu = menu_bar.addMenu("ë‚´ë³´ë‚´ê¸°")
        
        self.action_render = QAction("ì˜ìƒ ë Œë”ë§...", self)
        self.action_render.setShortcut(QKeySequence("F9"))
        self.action_render.triggered.connect(self._render_video)
        self.action_render.setEnabled(False)
        export_menu.addAction(self.action_render)

        export_menu.addSeparator()

        self.action_export_audio = QAction("ì˜¤ë””ì˜¤ ë‚´ë³´ë‚´ê¸°...", self)
        self.action_export_audio.triggered.connect(self._export_audio_dialog)
        self.action_export_audio.setEnabled(False)
        export_menu.addAction(self.action_export_audio)
        
        self.action_export_srt = QAction("SRT ìë§‰ ë‚´ë³´ë‚´ê¸°...", self)
        self.action_export_srt.triggered.connect(self._export_srt)
        self.action_export_srt.setEnabled(False)
        export_menu.addAction(self.action_export_srt)
        
        self.action_export_xml = QAction("XML í”„ë¡œì íŠ¸ ë‚´ë³´ë‚´ê¸°...", self)
        self.action_export_xml.triggered.connect(self._export_xml)
        self.action_export_xml.setEnabled(False)
        export_menu.addAction(self.action_export_xml)
    
    def _setup_ui(self):
        """Setup the main UI layout"""
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # --- Main Toolbar ---
        self._create_main_toolbar()
        
        main_layout = QVBoxLayout(central_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)
        
        # Main content area (splitter)
        splitter = QSplitter(Qt.Orientation.Horizontal)
        splitter.setHandleWidth(10)
        
        # Left panel - Script and speaker mapping
        left_panel = self._create_left_panel()
        splitter.addWidget(left_panel)
        
        # Right panel - Preview and timeline
        right_panel = self._create_right_panel()
        splitter.addWidget(right_panel)
        
        splitter.setSizes([400, 800])
        main_layout.addWidget(splitter, 1)
        
        # Status bar is already created by QMainWindow
        self.statusBar().showMessage("ì¤€ë¹„")
    
    def _create_main_toolbar(self):
        """Create the top main toolbar"""
        toolbar = QToolBar("Main Toolbar")
        toolbar.setMovable(False)
        toolbar.setFloatable(False)
        toolbar.setIconSize(QSize(20, 20))
        toolbar.setToolButtonStyle(Qt.ToolButtonStyle.ToolButtonTextBesideIcon)
        self.addToolBar(Qt.ToolBarArea.TopToolBarArea, toolbar)
        
        # Add actions
        toolbar.addAction(self.action_process)
        toolbar.addSeparator()
        toolbar.addAction(self.action_format_subs)
        toolbar.addAction(self.action_apply_images)
        
        # Spacer
        dummy = QWidget()
        dummy.setSizePolicy(
            QSizePolicy.Policy.Expanding,
            QSizePolicy.Policy.Expanding
        )
        toolbar.addWidget(dummy)
        
        # Right side actions
        toolbar.addAction(self.action_render)

    
    def _create_left_panel(self) -> QWidget:
        """Create left panel with script view and speaker mapping"""
        # Wrapper widget
        panel = QWidget()
        layout = QVBoxLayout(panel)
        layout.setContentsMargins(10, 5, 0, 0)  # Consistent margins
        
        # Vertical Splitter
        from PyQt6.QtWidgets import QSplitter
        splitter = QSplitter(Qt.Orientation.Vertical)
        splitter.setHandleWidth(10)
        
        # --- 1. Script Section ---
        script_group = QGroupBox("ìŠ¤í¬ë¦½íŠ¸")
        script_layout = QVBoxLayout(script_group)
        script_layout.setContentsMargins(10, 15, 10, 10) # Increased margins for consistency 

        self.script_text = QTextEdit()
        self.script_text.setReadOnly(True)
        self.script_text.setPlaceholderText("ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¤ì„¸ìš”...\n\nì§€ì› í˜•ì‹:\n* í™”ì: ëŒ€ì‚¬\n- í™”ì: ëŒ€ì‚¬\ní™”ì: ëŒ€ì‚¬")
        self.script_text.viewport().installEventFilter(self)
        
        script_layout.addWidget(self.script_text)
        splitter.addWidget(script_group)
        
        # --- 2. Speaker Mapping Section ---
        mapping_group = QGroupBox("í™”ìë³„ ì˜¤ë””ì˜¤")
        mapping_layout = QVBoxLayout(mapping_group)
        mapping_layout.setContentsMargins(10, 15, 10, 10) # Increased margins for consistency
        self.mapping_table = QTableWidget()
        self.mapping_table.setColumnCount(2)
        self.mapping_table.setHorizontalHeaderLabels(["í™”ì", "ì˜¤ë””ì˜¤ íŒŒì¼ (í´ë¦­í•˜ì—¬ íŒŒì¼ ì§€ì •)"])
        self.mapping_table.horizontalHeader().setSectionResizeMode(0, QHeaderView.ResizeMode.ResizeToContents)
        self.mapping_table.horizontalHeader().setSectionResizeMode(1, QHeaderView.ResizeMode.Stretch)
        self.mapping_table.setEditTriggers(QTableWidget.EditTrigger.NoEditTriggers)
        self.mapping_table.setFocusPolicy(Qt.FocusPolicy.NoFocus)
        self.mapping_table.cellClicked.connect(self._on_mapping_table_clicked)
        mapping_layout.addWidget(self.mapping_table)
        
        # Info label
        self.mapping_info = QLabel("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ë©´ í™”ì ëª©ë¡ì´ í‘œì‹œë©ë‹ˆë‹¤.")
        self.mapping_info.setStyleSheet("color: gray; font-style: italic;")
        mapping_layout.addWidget(self.mapping_info)
        
        splitter.addWidget(mapping_group)
        
        # --- 3. Image Files Section ---
        image_group = QGroupBox("ì´ë¯¸ì§€ íŒŒì¼")
        image_layout = QVBoxLayout(image_group)
        image_layout.setContentsMargins(10, 15, 10, 10) # Increased margins for consistency
        
        # Image list with thumbnails and drag-drop reordering
        self.image_list = DraggableImageListWidget()
        self.image_list.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAsNeeded)
        self.image_list.setIconSize(QSize(64, 64))  # Larger thumbnails
        self.image_list.setGridSize(QSize(100, 100))  # Match delegate sizeHint for consistent spacing
        # Use ListMode with wrapping for proper reordering behavior (IconMode allows free positioning)
        self.image_list.setViewMode(QListWidget.ViewMode.ListMode)
        self.image_list.setFlow(QListWidget.Flow.LeftToRight)
        self.image_list.setWrapping(True)
        self.image_list.setResizeMode(QListWidget.ResizeMode.Adjust)
        self.image_list.setSelectionMode(QListWidget.SelectionMode.ExtendedSelection)  # Multi-select with Ctrl/Shift
        self.image_list.setDragDropMode(QListWidget.DragDropMode.DragDrop)  # Enable drag to external widgets
        self.image_list.setDefaultDropAction(Qt.DropAction.MoveAction)
        self.image_list.setDragEnabled(True)
        self.image_list.setAcceptDrops(True)
        self.image_list.setFocusPolicy(Qt.FocusPolicy.NoFocus)
        
        # Use custom delegate to draw text BELOW icon
        self.image_list.setItemDelegate(ImageGridDelegate(self.image_list))
        
        # Connect zoom signal
        self.image_list.zoom_changed.connect(self._on_image_list_zoom_changed)

        # Install event filter for click-to-load
        self.image_list.viewport().installEventFilter(self)
        
        image_layout.addWidget(self.image_list)
        splitter.addWidget(image_group)
        
        # Set initial sizes
        splitter.setSizes([200, 300, 300])
        splitter.setCollapsible(0, False)
        splitter.setCollapsible(1, False)
        splitter.setCollapsible(2, False)
        
        layout.addWidget(splitter)
        
        return panel
    
    def _create_right_panel(self) -> QWidget:
        """Create right panel with preview and timeline (with splitter)"""
        # Wrapper widget to handle margins
        container = QWidget()
        layout = QVBoxLayout(container)
        layout.setContentsMargins(0, 5, 10, 0)  # Consistent margins with left panel
        layout.setSpacing(10)

        # Create a vertical splitter for preview and timeline
        from PyQt6.QtWidgets import QSplitter
        
        splitter = QSplitter(Qt.Orientation.Vertical)
        splitter.setHandleWidth(10)
        
        # Preview widget
        preview_container = QWidget()
        preview_layout = QVBoxLayout(preview_container)
        preview_layout.setContentsMargins(0, 0, 0, 0)
        
        preview_group = QGroupBox("ë¯¸ë¦¬ë³´ê¸°")
        preview_group_layout = QVBoxLayout(preview_group)
        self.preview_widget = PreviewWidget()
        
        # Connect preview position to timeline
        self.preview_widget.position_changed.connect(self._on_preview_position_changed)
        
        preview_group_layout.addWidget(self.preview_widget)
        preview_layout.addWidget(preview_group)
        
        splitter.addWidget(preview_container)
        
        # Timeline widget
        timeline_container = QWidget()
        timeline_layout = QVBoxLayout(timeline_container)
        timeline_layout.setContentsMargins(0, 0, 0, 0)
        
        timeline_group = QGroupBox("íƒ€ì„ë¼ì¸")
        timeline_group_layout = QVBoxLayout(timeline_group)
        self.timeline_widget = TimelineWidget()
        
        # Connect time sync signals
        self.timeline_widget.playhead_changed.connect(self._on_timeline_playhead_changed)
        
        # Connect clip edit signals
        self.timeline_widget.canvas.clip_editing.connect(self._on_clip_editing)
        self.timeline_widget.canvas.clip_edited.connect(self._on_clip_edited)
        self.timeline_widget.canvas.clip_moved.connect(self._on_clip_moved)
        self.timeline_widget.canvas.clip_double_clicked.connect(self._on_clip_double_clicked)
        self.timeline_widget.canvas.clip_context_menu.connect(self._on_clip_context_menu)
        self.timeline_widget.canvas.history_command_generated.connect(self._on_history_command)
        self.timeline_widget.canvas.image_dropped.connect(self._on_image_dropped)
        self.timeline_widget.canvas.clip_delete_requested.connect(self._on_clip_delete_requested)
        
        timeline_group_layout.addWidget(self.timeline_widget)
        timeline_layout.addWidget(timeline_group)
        
        splitter.addWidget(timeline_container)
        
        # Set initial sizes (preview smaller, timeline larger)
        splitter.setSizes([300, 200])
        
        layout.addWidget(splitter)
        return container

    def mousePressEvent(self, event):
        """Clear focus when clicking on empty space (background)"""
        # Get currently focused widget
        focused = QApplication.focusWidget()
        
        # If it's an input widget, clear focus
        if focused and isinstance(focused, (QLineEdit, QTextEdit, QPlainTextEdit, QSpinBox, QDoubleSpinBox)):
            focused.clearFocus()
            
        super().mousePressEvent(event)
    
    def keyPressEvent(self, event):
        """Handle global key press events"""
        # Spacebar to toggle playback, unless a text field is focused
        if event.key() == Qt.Key.Key_Space:
            focused = QApplication.focusWidget()
            # If focus is on a text input widget, allow standard behavior (typing space)
            if isinstance(focused, (QLineEdit, QTextEdit, QPlainTextEdit)):
                # If it's a read-only text widget, still allow playback toggle
                if hasattr(focused, 'isReadOnly') and not focused.isReadOnly():
                    super().keyPressEvent(event)
                    return

            # Otherwise toggle playback
            if hasattr(self, 'preview_widget'):
                self.preview_widget.toggle_playback()
                return

        super().keyPressEvent(event)

    def eventFilter(self, source, event):
        """Handle clicks on placeholders when empty"""
        if event.type() == QEvent.Type.MouseButtonRelease:
            if source is self.script_text.viewport():
                self._open_script_editor()
                return True
            elif source is self.image_list.viewport() and not self.image_folder:
                self._load_image_folder()
                return True
        return super().eventFilter(source, event)
    
    def _create_bottom_controls(self):
        """Create controls in status bar - Removed as they are now in Toolbar/Menu"""
        pass

    def _update_undo_redo_actions(self):
        """Update enabled state of Undo/Redo actions"""
        self.undo_action.setEnabled(self.undo_stack.can_undo())
        self.undo_action.setText(f"ì‹¤í–‰ ì·¨ì†Œ({self.undo_stack.undo_stack[-1].text()})" if self.undo_stack.can_undo() else "ì‹¤í–‰ ì·¨ì†Œ")
        self.redo_action.setEnabled(self.undo_stack.can_redo())
        self.redo_action.setText(f"ë‹¤ì‹œ ì‹¤í–‰({self.undo_stack.redo_stack[-1].text()})" if self.undo_stack.can_redo() else "ë‹¤ì‹œ ì‹¤í–‰")

        # Also update window title to show modified status
        self._update_title()

    def mark_modified(self):
        """Mark the project as manually modified (for non-undoable actions)"""
        self._manual_modification_flag = True
        self._update_title()

    def mark_clean(self):
        """Mark the project as clean (saved)"""
        self.undo_stack.set_clean()
        self._manual_modification_flag = False
        self._update_title()

    def is_modified(self) -> bool:
        """Check if project has unsaved changes"""
        return (not self.undo_stack.is_clean()) or self._manual_modification_flag

    def _update_title(self):
        """Update window title with modified status"""
        title = "PictureBookBuilder"
        if self.project_path:
            title += f" - {Path(self.project_path).name}"

        if self.is_modified():
            title += " *"

        self.setWindowTitle(title)

    def _undo(self):
        """Undo last action"""
        if self.undo_stack.can_undo():
            text = self.undo_stack.undo()
            self._update_undo_redo_actions()
            self.statusBar().showMessage(f"ì‹¤í–‰ ì·¨ì†Œë¨: {text}")

    def _redo(self):
        """Redo last action"""
        if self.undo_stack.can_redo():
            text = self.undo_stack.redo()
            self._update_undo_redo_actions()
            self.statusBar().showMessage(f"ë‹¤ì‹œ ì‹¤í–‰ë¨: {text}")

    def _on_undo_redo_callback(self):
        """Callback after undo/redo to refresh UI"""
        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        self.timeline_widget.canvas._update_total_duration()

        # Sync to preview and regenerate (set data first, then audio)
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        self._regenerate_preview_from_clips() # Might be heavy but ensures audio sync

    def _on_history_command(self, action_type, data):
        """Handle history command generation from TimelineCanvas"""
        cmd = None
        if action_type == 'modify':
            cmd = ModifyClipsCommand(
                self.timeline_widget.canvas,
                data['modifications'],
                data['description'],
                callback=self._on_undo_redo_callback
            )

        if cmd:
            self.undo_stack.push(cmd)
            self._update_undo_redo_actions()
    
    def _load_script(self):
        """Load script file and detect speakers"""
        path, _ = QFileDialog.getOpenFileName(
            self, "ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì„ íƒ", "", "Text Files (*.txt);;All Files (*)"
        )
        if path:
            self.script_path = path
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
                self.script_text.setText(content)
            
            # Parse script to detect speakers
            self._detect_speakers()
            self._check_ready()
            self.mark_modified()
    
    def _open_script_editor(self):
        """Open a larger dialog to edit the script"""
        dialog = QDialog(self)
        dialog.setWindowTitle("ìŠ¤í¬ë¦½íŠ¸ í¸ì§‘")
        dialog.resize(800, 600)

        layout = QVBoxLayout(dialog)

        # Text Editor
        editor = QTextEdit()
        editor.setPlainText(self.script_text.toPlainText())
        # Set larger font for better visibility
        font = editor.font()
        font.setPointSize(12)
        editor.setFont(font)
        layout.addWidget(editor)

        # Buttons
        btn_layout = QHBoxLayout()

        load_btn = QPushButton("ë¶ˆëŸ¬ì˜¤ê¸°...")
        def load_file_content():
            path, _ = QFileDialog.getOpenFileName(
                dialog, "ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì„ íƒ", "", "Text Files (*.txt);;All Files (*)"
            )
            if path:
                self.script_path = path  # Update main window path tracking
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    editor.setPlainText(content)
        load_btn.clicked.connect(load_file_content)

        save_btn = QPushButton("ì €ì¥")
        save_btn.setDefault(True)
        cancel_btn = QPushButton("ì·¨ì†Œ")

        save_btn.clicked.connect(dialog.accept)
        cancel_btn.clicked.connect(dialog.reject)

        btn_layout.addWidget(load_btn)
        btn_layout.addStretch()
        btn_layout.addWidget(save_btn)
        btn_layout.addWidget(cancel_btn)
        layout.addLayout(btn_layout)

        if dialog.exec() == QDialog.DialogCode.Accepted:
            new_text = editor.toPlainText()
            self.script_text.setPlainText(new_text)
            self._detect_speakers()
            self.statusBar().showMessage("ìŠ¤í¬ë¦½íŠ¸ê°€ ì—…ë°ì´íŠ¸ë˜ê³  í™”ìê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            self._check_ready()

    def _detect_speakers(self):
        """Detect speakers from script and update mapping table"""
        text = self.script_text.toPlainText()
        if not text.strip():
            return
        
        from core.script_parser import ScriptParser
        parser = ScriptParser()
        dialogues = parser.parse_text(text)
        self.speakers = parser.get_unique_speakers(dialogues)
        
        # Update mapping table
        self.mapping_table.setRowCount(len(self.speakers))
        self.speaker_audio_map = {}
        
        for i, speaker in enumerate(self.speakers):
            # Speaker name
            speaker_item = QTableWidgetItem(speaker)
            speaker_item.setFlags(speaker_item.flags() & ~Qt.ItemFlag.ItemIsEditable)
            self.mapping_table.setItem(i, 0, speaker_item)
            
            # Audio file (empty initially)
            audio_item = QTableWidgetItem("(ì„ íƒ ì•ˆë¨)")
            audio_item.setForeground(QColor(150, 150, 150))
            self.mapping_table.setItem(i, 1, audio_item)
            
            self.speaker_audio_map[speaker] = ""
        
        # Enable grid lines for better visibility
        self.mapping_table.setShowGrid(True)
        self.mapping_table.setStyleSheet("QTableWidget::item { border-bottom: 1px solid #333333; }")
        
        # Update info
        self.mapping_info.setText(f"{len(self.speakers)}ëª…ì˜ í™”ì ê°ì§€ë¨. ê° í™”ìì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì§€ì •í•˜ì„¸ìš”.")
        self.mapping_info.setStyleSheet("color: orange;")

        # Update canvas map
        self.timeline_widget.canvas.speaker_audio_map = self.speaker_audio_map
    
    def _on_mapping_table_clicked(self, row, column):
        """Handle click on speaker mapping table"""
        speaker = self.mapping_table.item(row, 0).text()
        self._select_audio_for_speaker(speaker, row)
        
    def _select_audio_for_speaker(self, speaker: str, row: int):
        """Open file dialog to select audio for a specific speaker"""
        path, _ = QFileDialog.getOpenFileName(
            self, f"'{speaker}' ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ", "",
            "Audio Files (*.wav *.mp3 *.m4a *.ogg);;All Files (*)"
        )
        if path:
            self.speaker_audio_map[speaker] = path
            
            # Update canvas map and redraw to clear any missing file warnings
            self.timeline_widget.canvas.speaker_audio_map = self.speaker_audio_map
            self.timeline_widget.canvas._background_dirty = True
            self.timeline_widget.canvas.update()

            # Update table
            audio_item = QTableWidgetItem(Path(path).name)
            audio_item.setForeground(QColor(100, 200, 100))
            self.mapping_table.setItem(row, 1, audio_item)
            
            # Check if all speakers have audio
            self._update_mapping_status()
            self._check_ready()
            self.mark_modified()
    
    def _update_mapping_status(self):
        """Update the mapping info label based on current state"""
        mapped = sum(1 for v in self.speaker_audio_map.values() if v)
        total = len(self.speakers)
        
        if mapped == total:
            self.mapping_info.setText(f"ëª¨ë“  í™”ì({total}ëª…)ì— ì˜¤ë””ì˜¤ê°€ ì§€ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
            self.mapping_info.setStyleSheet("color: green;")
        else:
            self.mapping_info.setText(f"{mapped}/{total}ëª… ì§€ì •ë¨. ëª¨ë“  í™”ìì— ì˜¤ë””ì˜¤ë¥¼ ì§€ì •í•˜ì„¸ìš”.")
            self.mapping_info.setStyleSheet("color: orange;")
    
    def _load_image_folder(self):
        """Load image folder"""
        path = QFileDialog.getExistingDirectory(self, "ì´ë¯¸ì§€ í´ë” ì„ íƒ")
        if path:
            self.image_folder = path
            
            # Setup file watcher
            if self.image_watcher.directories():
                self.image_watcher.removePaths(self.image_watcher.directories())
            self.image_watcher.addPath(path)
            
            self._populate_image_list(path)
            
            # Enable reload action
            self.reload_images_action.setEnabled(True)
            
            # If processing is already done, enable apply button
            if self.timeline_widget.canvas.clips:
                self.action_apply_images.setEnabled(True)
                self.action_auteur_import.setEnabled(True)

            self.mark_modified()
    
    def _reload_image_folder(self):
        """Reload images from the current image folder"""
        if self.image_folder:
            # Clear cache to force reload (in case files changed)
            from .image_cache import get_image_cache
            get_image_cache().clear()
            
            self._populate_image_list(self.image_folder)
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ í´ë”ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {self.image_folder}")
    
    def _on_directory_changed(self, path):
        """Handle directory change notification"""
        # Restart timer to debounce (wait for copy operations to finish)
        self.image_update_timer.start(1000)
    
    def _on_image_update_timeout(self):
        """Update image list after debounce"""
        if self.image_folder and os.path.exists(self.image_folder):
            # Save current selection if possible (by filename)
            selected_items = self.image_list.selectedItems()
            selected_files = [item.data(Qt.ItemDataRole.UserRole) for item in selected_items]
            
            self._populate_image_list(self.image_folder)
            
            # Restore selection
            if selected_files:
                for i in range(self.image_list.count()):
                    item = self.image_list.item(i)
                    if item.data(Qt.ItemDataRole.UserRole) in selected_files:
                        item.setSelected(True)
            
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ ëª©ë¡ì´ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.", 3000)
    
    def _populate_image_list(self, folder_path: str):
        """Populate image list with thumbnails (loads all upfront, natural sorting)"""
        import re
        from .image_cache import get_image_cache
        
        def natural_key(text):
            return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\d+)', text)]

        self.image_list.clear()
        image_path = Path(folder_path)
        images = []
        for ext in ['*.png', '*.jpg', '*.jpeg', '*.webp']:
            images.extend(image_path.glob(ext))
        
        # Sort images naturally (1, 2, 10 instead of 1, 10, 2)
        images.sort(key=lambda x: natural_key(x.name))
        
        # Get image cache
        cache = get_image_cache()
        
        # Disconnect previous signal if connected
        try:
            cache.image_loaded.disconnect(self._on_thumbnail_ready)
        except TypeError:
            pass  # Not connected
        
        # Connect signal for thumbnail updates
        cache.image_loaded.connect(self._on_thumbnail_ready)
        
        # Store path to item mapping for updates
        self._image_path_to_item: dict[str, QListWidgetItem] = {}
        
        # Add items with placeholder icons immediately
        image_paths_to_load = []
        for f in images:
            path_str = str(f)
            
            # Create item (will set icon below if cached)
            item = QListWidgetItem(f"ğŸ–¼ï¸ {f.name}")
            item.setData(Qt.ItemDataRole.UserRole, path_str)
            self.image_list.addItem(item)
            
            # Store mapping for async update
            self._image_path_to_item[path_str] = item
            
            # Check if already cached - if so, apply thumbnail immediately
            if cache.has_thumbnail(path_str):
                self._update_item_icon(item, path_str)
            else:
                # Need to load this image
                image_paths_to_load.append(path_str)
        
        # Load only images not already in cache
        if image_paths_to_load:
            cache.load_images(image_paths_to_load)
    
    def _on_thumbnail_ready(self, path: str):
        """Handle image load completion - update list item with thumbnail"""
        if not hasattr(self, '_image_path_to_item'):
            return
        
        item = self._image_path_to_item.get(path)
        if item:
            self._update_item_icon(item, path)
        
        # Also update timeline if this image is used there
        for clip in self.timeline_widget.canvas.clips:
            if clip.clip_type == "image" and clip.image_path == path:
                self.timeline_widget.canvas._background_dirty = True
                self.timeline_widget.canvas.update()
                break
    
    def _on_image_list_zoom_changed(self, new_size: int):
        """Handle zoom change in image list - update icons to appropriate resolution"""
        for i in range(self.image_list.count()):
            item = self.image_list.item(i)
            path = item.data(Qt.ItemDataRole.UserRole)
            if path:
                self._update_item_icon(item, path)

    def _update_item_icon(self, item: QListWidgetItem, path: str):
        """Update item icon using best available thumbnail for current size"""
        from .image_cache import get_image_cache
        cache = get_image_cache()

        current_size = self.image_list.iconSize().width()

        # Select appropriate thumbnail based on size
        pixmap = None
        if current_size > 50:
             # Use larger preview thumbnail for zoomed in view
             pixmap = cache.get_thumbnail_preview(path)
             # Fallback to timeline if preview not available
             if not pixmap:
                 pixmap = cache.get_thumbnail_timeline(path)

        # Fallback to small or if size is small
        if not pixmap:
            pixmap = cache.get_thumbnail_small(path)

        if pixmap and not pixmap.isNull():
            item.setIcon(QIcon(pixmap))
            item.setText(Path(path).name)

    def _apply_images_to_timeline(self):
        """Apply images from list to timeline, mapping 1:1 with audio clips"""
        
        # Get audio clips from timeline
        audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
        if not audio_clips:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "íƒ€ì„ë¼ì¸ì— ì˜¤ë””ì˜¤ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Get images from list (in current order)
        image_paths = []
        for i in range(self.image_list.count()):
            item = self.image_list.item(i)
            path = item.data(Qt.ItemDataRole.UserRole)
            if path:
                image_paths.append(path)
        
        if not image_paths:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ì´ë¯¸ì§€ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        # Sort audio clips by start time
        audio_clips_sorted = sorted(audio_clips, key=lambda c: c.start)
        
        # Check for surplus images and ask user what to do
        surplus_count = len(image_paths) - len(audio_clips_sorted)
        append_surplus = False
        
        if surplus_count > 0:
            msg = QMessageBox(self)
            msg.setIcon(QMessageBox.Icon.Question)
            msg.setWindowTitle("ì´ë¯¸ì§€ ë‚¨ìŒ")
            msg.setText(f"ì´ë¯¸ì§€ê°€ {surplus_count}ê°œ ë‚¨ìŠµë‹ˆë‹¤.")
            msg.setInformativeText(f"ì´ ì´ë¯¸ì§€: {len(image_paths)}ê°œ\nì˜¤ë””ì˜¤ í´ë¦½: {len(audio_clips_sorted)}ê°œ")
            
            ignore_btn = msg.addButton("ë¬´ì‹œí•˜ê¸°", QMessageBox.ButtonRole.RejectRole)
            append_btn = msg.addButton("ë’¤ì— ì¶”ê°€", QMessageBox.ButtonRole.AcceptRole)
            msg.setDefaultButton(ignore_btn)
            
            msg.exec()
            
            if msg.clickedButton() == append_btn:
                append_surplus = True
        
        # Check if there are existing image clips
        existing_image_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "image"]
        if existing_image_clips:
            reply = QMessageBox.warning(
                self, "ê²½ê³ ",
                f"ê¸°ì¡´ ì´ë¯¸ì§€ í´ë¦½ {len(existing_image_clips)}ê°œê°€ ëª¨ë‘ ì‚­ì œë©ë‹ˆë‹¤.\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                QMessageBox.StandardButton.No
            )
            if reply != QMessageBox.StandardButton.Yes:
                return
        
        new_image_clips = []
        
        # Map images to audio clips 1:1
        for i, audio_clip in enumerate(audio_clips_sorted):
            if i < len(image_paths):
                img_path = image_paths[i]
            else:
                # No more images - leave blank (no image clip)
                continue
            
            # Calculate image duration
            img_start = audio_clip.start
            if i + 1 < len(audio_clips_sorted):
                img_end = audio_clips_sorted[i + 1].start
            else:
                img_end = audio_clip.start + audio_clip.duration
            
            img_clip = TimelineClip(
                id=self._make_unique_clip_id(f"img_{i}"),
                name=Path(img_path).name,
                start=img_start,
                duration=img_end - img_start,
                track=2,  # Image track
                color=QColor("#9E9E9E"),
                clip_type="image",
                waveform=[],
                image_path=img_path
            )
            new_image_clips.append(img_clip)
        
        # Append surplus images if user requested
        if append_surplus and surplus_count > 0:
            # Calculate where to start appending (after last audio clip)
            if audio_clips_sorted:
                last_audio = audio_clips_sorted[-1]
                append_start = last_audio.start + last_audio.duration
            else:
                append_start = 0.0
            
            # Add surplus images with 5 second duration each
            for i in range(len(audio_clips_sorted), len(image_paths)):
                img_path = image_paths[i]
                img_duration = 5.0  # 5 seconds default
                
                img_clip = TimelineClip(
                    id=self._make_unique_clip_id(f"img_{i}"),
                    name=Path(img_path).name,
                    start=append_start,
                    duration=img_duration,
                    track=2,
                    color=QColor("#9E9E9E"),
                    clip_type="image",
                    waveform=[],
                    image_path=img_path
                )
                new_image_clips.append(img_clip)
                append_start += img_duration

        # Create undo command
        cmd = AddRemoveClipsCommand(
            self.timeline_widget.canvas,
            added=new_image_clips,
            removed=existing_image_clips,
            description="Apply images to timeline",
            callback=self._on_undo_redo_callback
        )
        self.undo_stack.push(cmd)
        cmd.redo()
        self._update_undo_redo_actions()

        # Update timeline
        self.timeline_widget.canvas._update_total_duration()
        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        
        # Sync to preview
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips)
        
        # Show result
        applied = len(new_image_clips)
        if append_surplus and surplus_count > 0:
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ {applied}ê°œê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ({surplus_count}ê°œ ë’¤ì— ì¶”ê°€ë¨)")
        else:
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ {applied}ê°œê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _check_ready(self):
        """Check if we have all inputs to start processing"""
        # Need script and all speakers mapped
        all_mapped = all(self.speaker_audio_map.get(s) for s in self.speakers)
        ready = bool(self.script_text.toPlainText().strip() and self.speakers and all_mapped)
        self.action_process.setEnabled(ready)
    
    def _start_processing(self):
        """Start the processing thread with progress dialog"""
        self.action_process.setEnabled(False)
        
        # Create and show progress dialog
        self.progress_dialog = ProgressDialog(self, "ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")
        
        self.processing_thread = ProcessingThread(
            self.script_text.toPlainText(),
            self.speaker_audio_map.copy(),
            self.image_folder or ""
        )
        
        # Connect signals
        self.processing_thread.progress.connect(self._on_progress)
        self.processing_thread.finished.connect(self._on_processing_finished)
        self.progress_dialog.cancelled.connect(self.processing_thread.cancel)
        
        self.processing_thread.start()
        self.progress_dialog.show()
    
    def _on_progress(self, percent: int, message: str):
        """Handle progress updates"""
        self.statusBar().showMessage(f"{message} ({percent}%)")
        if hasattr(self, 'progress_dialog') and self.progress_dialog:
            self.progress_dialog.update_progress(percent, message)
    
    def _on_processing_finished(self, success: bool, message: str, result: Optional[dict]):
        """Handle processing completion"""
        # Close progress dialog
        if hasattr(self, 'progress_dialog') and self.progress_dialog:
            self.progress_dialog._is_cancelled = True  # Allow closing
            self.progress_dialog.close()
            self.progress_dialog = None
        
        self.action_process.setEnabled(True)
        self.result_data = result
        
        if success:
            self.statusBar().showMessage("ì²˜ë¦¬ ì™„ë£Œ")
            self.action_export_srt.setEnabled(True)
            self.action_export_xml.setEnabled(True)
            self.action_render.setEnabled(True)
            
            # Update timeline with aligned clips
            if result and 'aligned' in result:
                self._update_timeline(result)
                # Generate preview audio
                self._generate_preview_audio(result)
                
                # Enable image apply button if we have images
                if self.image_list.count() > 0:
                    self.action_apply_images.setEnabled(True)
                    self.action_auteur_import.setEnabled(True)
            
            QMessageBox.information(self, "ì™„ë£Œ", message)
        else:
            self.statusBar().showMessage("ì·¨ì†Œë¨" if "ì·¨ì†Œ" in message else "ì˜¤ë¥˜ ë°œìƒ")
            if "ì·¨ì†Œ" not in message:
                QMessageBox.critical(self, "ì˜¤ë¥˜", message)
    
    def _update_timeline(self, result: dict):
        """Update the timeline with aligned segments and waveforms"""
        from .timeline_widget import TimelineClip
        from pydub import AudioSegment
        import numpy as np
        
        aligned = result.get('aligned', [])
        speaker_audio_map = result.get('speaker_audio_map', {})
        clips = []
        current_time = 0.0
        current_time = 0.0
        gap = DEFAULT_GAP_SECONDS
        
        # Load speaker audio files for waveform extraction
        speaker_audio: dict[str, AudioSegment] = {}
        for speaker, audio_path in speaker_audio_map.items():
            if audio_path:
                try:
                    speaker_audio[speaker] = AudioSegment.from_file(audio_path)
                except:
                    pass
        
        for i, segment in enumerate(aligned):
            duration = segment.end_time - segment.start_time
            
            # Extract waveform data
            waveform = []
            speaker = segment.dialogue.speaker
            actual_duration = duration
            
            if speaker in speaker_audio:
                audio = speaker_audio[speaker]
                
                start_ms = max(0, int(segment.start_time * 1000))
                end_ms = min(len(audio), int(segment.end_time * 1000))
                clip_audio = audio[start_ms:end_ms]
                
                # Use actual extracted clip duration for timeline sync
                actual_duration = len(clip_audio) / 1000.0
                
                waveform = self._extract_waveform_from_audio(clip_audio)
            
            # 1. Audio clip (Track 0)

            audio_clip = TimelineClip(
                id=f"audio_{i}",
                name="", # Remove text from audio clip for cleaner look
                start=current_time,
                duration=actual_duration,
                track=0,
                color=self.timeline_widget.canvas.get_color_for_speaker(segment.dialogue.speaker),
                clip_type="audio",
                waveform=waveform,
                offset=segment.start_time,  # Store segment start time
                segment_index=i,
                speaker=segment.dialogue.speaker
            )
            clips.append(audio_clip)
            
            # 2. Subtitle clip (Track 1)
            sub_clip = TimelineClip(
                id=f"sub_{i}",
                name=segment.dialogue.text, # Full text here
                start=current_time,
                duration=actual_duration,
                track=1,
                color=QColor(self.timeline_widget.canvas.get_color_for_speaker(segment.dialogue.speaker)).lighter(150),
                clip_type="subtitle", # New type
                waveform=[],
                offset=segment.start_time,  # Store VAD-adjusted start time
                segment_index=i,
                speaker=segment.dialogue.speaker,
                words=segment.words or []  # Pass word timestamps for editing
            )
            clips.append(sub_clip)
            
            current_time += actual_duration + gap
        
        # Add image clips to track 1 (synced with audio clips)
        if self.image_folder:
            from pathlib import Path as P
            image_folder = P(self.image_folder)
            images = []
            for ext in ['*.png', '*.jpg', '*.jpeg', '*.webp']:
                images.extend(sorted(image_folder.glob(ext)))
            
            if images:
                # Get audio clip start times for image sync
                audio_clip_starts = [c.start for c in clips]
                audio_clip_ends = [c.start + c.duration for c in clips]
                
                # Number of images to use (match with audio clips or fewer)
                num_images_to_use = min(len(images), len(clips))
                
                for i in range(num_images_to_use):
                    img_path = images[i]
                    
                    # Image starts when audio clip starts
                    img_start = audio_clip_starts[i]
                    
                    # Image ends when next audio clip starts (or at the end)
                    if i + 1 < len(audio_clip_starts):
                        img_end = audio_clip_starts[i + 1]
                    else:
                        img_end = audio_clip_ends[i]
                    
                    img_duration = img_end - img_start
                    
                    img_clip = TimelineClip(
                        id=f"img_{i}",
                        name=img_path.name,
                        start=img_start,
                        duration=img_duration,
                        track=2,  # Image track moved to 2
                        color=QColor("#9E9E9E"),  # Gray color for images
                        clip_type="image",
                        waveform=[],
                        image_path=str(img_path)
                    )
                    clips.append(img_clip)
        
        self.timeline_widget.set_clips(clips)
        
        # Sync to preview widget
        self.preview_widget.set_timeline_clips(clips)
        
        # Enable subtitle formatting button
        self.action_format_subs.setEnabled(True)
    
    def _extract_waveform_from_audio(self, audio_segment) -> list[float]:
        """Extract normalized waveform data from an audio segment"""
        import numpy as np
        
        # Convert to numpy array
        samples = np.array(audio_segment.get_array_of_samples())
        waveform = []
        
        if len(samples) > 0:
            # Downsample for display - use duration-based sampling for consistent resolution
            # 100 samples per second ensures all clips have similar visual density
            duration_seconds = len(audio_segment) / 1000.0  # pydub uses milliseconds
            samples_per_second = 100
            target_samples = max(50, int(duration_seconds * samples_per_second))
            if len(samples) > target_samples:
                chunk_size = len(samples) // target_samples
                downsampled = []
                for j in range(0, len(samples), chunk_size):
                    chunk = samples[j:j+chunk_size]
                    if len(chunk) > 0:
                        # Take max absolute value for each chunk
                        downsampled.append(float(np.max(np.abs(chunk))))
                waveform = downsampled
            else:
                waveform = [float(abs(s)) for s in samples]
            
            # Normalize to 0-1 range
            if waveform:
                max_val = max(waveform) if max(waveform) > 0 else 1
                waveform = [v / max_val for v in waveform]
        
        return waveform
    
    def _generate_preview_audio(self, result: dict):
        """Setup AudioMixer with clips for preview playback"""
        try:
            from .audio_mixer import ScheduledClip
            
            aligned = result.get('aligned', [])
            speaker_audio_map = result.get('speaker_audio_map', {})
            
            if not aligned:
                return
            
            self.statusBar().showMessage("ë¯¸ë¦¬ë³´ê¸° ì˜¤ë””ì˜¤ ì¤€ë¹„ ì¤‘...")
            
            # Build scheduled clips from timeline clips
            scheduled_clips = []
            audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
            
            for clip in audio_clips:
                scheduled_clips.append(ScheduledClip(
                    clip_id=clip.id,
                    speaker=clip.speaker,
                    timeline_start=clip.start,
                    timeline_end=clip.start + clip.duration,
                    source_offset=clip.offset,
                    source_path=speaker_audio_map.get(clip.speaker, ""),
                    duration=clip.duration,
                    volume=clip.volume
                ))
            
            # Set up the AudioMixer
            self.preview_widget.set_audio_clips(scheduled_clips, speaker_audio_map)
            
            # Calculate total timeline duration from ALL clips
            all_clips = self.timeline_widget.canvas.clips
            total_duration = max((c.start + c.duration for c in all_clips), default=0.0)
            self.preview_widget.set_total_duration(total_duration)
            
            # Connect preview position to timeline playhead (only once)
            if not hasattr(self, '_preview_connected') or not self._preview_connected:
                self.preview_widget.audio_mixer.position_changed.connect(self._on_preview_position_changed)
                self._preview_connected = True
            
            # Sync timeline clips to preview for image/subtitle display
            self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips)
            
            self.statusBar().showMessage("ë¯¸ë¦¬ë³´ê¸° ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.statusBar().showMessage(f"ë¯¸ë¦¬ë³´ê¸° ì˜¤ë””ì˜¤ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}")
    
    def _on_timeline_playhead_changed(self, time: float):
        """Handle playhead change from timeline - sync to preview"""
        if hasattr(self, 'preview_widget') and self.preview_widget.audio_path:
            # Convert time to milliseconds and seek preview
            position_ms = int(time * 1000)
            self.preview_widget.audio_mixer.seek(time)
    
    def _on_preview_position_changed(self, position_ms: int):
        """Handle position change from preview - sync to timeline"""
        if hasattr(self, 'timeline_widget'):
            time_sec = position_ms / 1000.0
            # Only auto-scroll when playing
            is_playing = self.preview_widget.audio_mixer.is_playing
            self.timeline_widget.set_playhead(time_sec, auto_scroll=is_playing)
    
    
    def _on_clip_editing(self, clip_id: str):
        """Handle real-time clip boundary change - update waveform and AudioMixer"""
        # Find the editing clip
        clip = None
        for c in self.timeline_widget.canvas.clips:
            if c.id == clip_id:
                clip = c
                break
        if not clip: return

        # Sync preview in real-time (preserve current playhead position)
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)

        # Only audio clips should generate/display waveforms.
        if getattr(clip, 'clip_type', None) != "audio":
            return
        
        # Update AudioMixer with the modified clip in real-time
        from .audio_mixer import ScheduledClip
        speaker_audio_map = self.result_data.get('speaker_audio_map', {}) if self.result_data else self.speaker_audio_map
        
        scheduled_clip = ScheduledClip(
            clip_id=clip.id,
            speaker=clip.speaker,
            timeline_start=clip.start,
            timeline_end=clip.start + clip.duration,
            source_offset=clip.offset,
            source_path=speaker_audio_map.get(clip.speaker, ""),
            duration=clip.duration,
            volume=clip.volume
        )
        self.preview_widget.update_audio_clip(scheduled_clip)

        # Update waveform only (fast path) using offset + duration
        try:
            # Check cache for speaker audio
            if clip.speaker not in self.speaker_audio_cache:
                audio_path = speaker_audio_map.get(clip.speaker)
                if audio_path:
                    from pydub import AudioSegment
                    self.speaker_audio_cache[clip.speaker] = AudioSegment.from_file(audio_path)
            
            audio = self.speaker_audio_cache.get(clip.speaker)
            if audio:
                # Extract audio using offset and duration
                segment_end = clip.offset + clip.duration
                
                start_ms = max(0, int(clip.offset * 1000))
                end_ms = min(len(audio), int(segment_end * 1000))
                
                # Simple cache for waveform itself during drag to prevent flickering
                cache_key = f"{clip_id}_{start_ms}_{end_ms}"
                if cache_key in self._waveform_cache:
                    clip.waveform = self._waveform_cache[cache_key]
                else:
                    clip_audio = audio[start_ms:end_ms]
                    clip.waveform = self._extract_waveform_from_audio(clip_audio)
                    self._waveform_cache[cache_key] = clip.waveform
                
                self.timeline_widget.canvas.update()
        except Exception as e:
            pass

    def _on_clip_edited(self, clip_id: str):
        """Handle final clip boundary edit - sync to result_data and update AudioMixer"""
        # Find the edited clip
        clip = None
        for c in self.timeline_widget.canvas.clips:
            if c.id == clip_id:
                clip = c
                break
        
        if not clip:
            return
        
        # Audio-specific: Update aligned segment data using offset + duration
        if clip.clip_type == "audio" and clip.segment_index >= 0 and self.result_data:
            aligned = self.result_data.get('aligned', [])
            if clip.segment_index < len(aligned):
                segment = aligned[clip.segment_index]
                # Update segment times based on clip's offset and duration
                segment.start_time = clip.offset
                segment.end_time = clip.offset + clip.duration
                self.statusBar().showMessage(f"ì˜¤ë””ì˜¤ í´ë¦½ ìˆ˜ì •ë¨: {clip.offset:.2f}s ~ {clip.source_end:.2f}s")
        else:
            self.statusBar().showMessage(f"í´ë¦½ ìˆ˜ì •ë¨: {clip.name}")
        
        # Sync to preview widget for ALL types
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        
        # Update AudioMixer for audio clips (already updated in _on_clip_editing, but finalize here)
        if clip.clip_type == "audio":
            from .audio_mixer import ScheduledClip
            speaker_audio_map = self.result_data.get('speaker_audio_map', {}) if self.result_data else self.speaker_audio_map
            
            scheduled_clip = ScheduledClip(
                clip_id=clip.id,
                speaker=clip.speaker,
                timeline_start=clip.start,
                timeline_end=clip.start + clip.duration,
                source_offset=clip.offset,
                source_path=speaker_audio_map.get(clip.speaker, ""),
                duration=clip.duration,
                volume=clip.volume
            )
            self.preview_widget.update_audio_clip(scheduled_clip)
            
        # Update total duration for ALL clip types
        all_clips = self.timeline_widget.canvas.clips
        total_duration = max((c.start + c.duration for c in all_clips), default=0.0)
        self.preview_widget.set_total_duration(total_duration)
    
    def _on_clip_moved(self, clip_id: str, new_start: float):
        """Handle clip position change - update ALL audio clips for ripple edit support"""
        self.statusBar().showMessage(f"í´ë¦½ ì´ë™ë¨: {new_start:.2f}s")
        
        # Sync to preview widget
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        
        # Update AudioMixer for ALL audio clips (required for ripple edit)
        # Ripple edit moves multiple clips, so we need to update all audio clip positions
        from .audio_mixer import ScheduledClip
        speaker_audio_map = self.result_data.get('speaker_audio_map', {}) if self.result_data else self.speaker_audio_map
        
        for clip in self.timeline_widget.canvas.clips:
            if clip.clip_type == "audio":
                scheduled_clip = ScheduledClip(
                    clip_id=clip.id,
                    speaker=clip.speaker,
                    timeline_start=clip.start,
                    timeline_end=clip.start + clip.duration,
                    source_offset=clip.offset,
                    source_path=speaker_audio_map.get(clip.speaker, ""),
                    duration=clip.duration,
                    volume=clip.volume
                )
                self.preview_widget.update_audio_clip(scheduled_clip)
            
        # Update total duration for ALL clip types
        all_clips = self.timeline_widget.canvas.clips
        total_duration = max((c.start + c.duration for c in all_clips), default=0.0)
        self.preview_widget.set_total_duration(total_duration)
    
    def _on_clip_double_clicked(self, clip_id: str):
        """Handle clip double click - edit subtitle text"""
        # Find the clip
        clip = None
        for c in self.timeline_widget.canvas.clips:
            if c.id == clip_id:
                clip = c
                break
        
        if not clip or clip.clip_type != "subtitle":
            return
            
        # Open dialog to edit text
        from PyQt6.QtWidgets import QInputDialog
        new_text, ok = QInputDialog.getMultiLineText(
            self, "ìë§‰ ìˆ˜ì •", "ëŒ€ì‚¬ ë‚´ìš©ì„ ìˆ˜ì •í•˜ì„¸ìš”:", clip.name
        )
        
        if ok and new_text:
            old_state = copy.deepcopy(clip)

            clip.name = new_text
            # Also update the actual data
            if self.result_data and clip.segment_index >= 0:
                aligned = self.result_data.get('aligned', [])
                if clip.segment_index < len(aligned):
                    aligned[clip.segment_index].dialogue.text = new_text
            
            new_state = copy.deepcopy(clip)

            # Undo Command
            cmd = ModifyClipsCommand(
                self.timeline_widget.canvas,
                [(clip.id, old_state, new_state)],
                description="Edit subtitle text",
                callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(cmd)
            self._update_undo_redo_actions()

            self.timeline_widget.canvas._background_dirty = True
            self.timeline_widget.canvas.update()
            self.statusBar().showMessage("ìë§‰ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # Sync to preview immediately
            playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
            self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
    
    def _on_clip_context_menu(self, clip_id: str, pos):
        """Show context menu for clip operations"""
        from PyQt6.QtWidgets import QMenu
        
        # Find the clip
        clip = None
        for c in self.timeline_widget.canvas.clips:
            if c.id == clip_id:
                clip = c
                break
        
        if not clip:
            return
        
        menu = QMenu(self)
        
        if clip.clip_type == "subtitle":
            edit_action = menu.addAction("í…ìŠ¤íŠ¸ ìˆ˜ì •")
            split_action = menu.addAction("ìë§‰ ë‚˜ëˆ„ê¸°...")
            menu.addSeparator()
            
            # Find if there's a next subtitle clip
            next_clip = self._find_adjacent_subtitle(clip, direction=1)
            merge_action = None
            if next_clip:
                merge_action = menu.addAction("ë‹¤ìŒ ìë§‰ê³¼ ë³‘í•©")
            
            action = menu.exec(pos)
            
            if action == edit_action:
                self._on_clip_double_clicked(clip_id)
            elif action == split_action:
                self._show_subtitle_editor(clip)
            elif merge_action and action == merge_action:
                self._merge_subtitle_clips(clip, next_clip)
        
        elif clip.clip_type == "image":
            change_image_action = menu.addAction("ì´ë¯¸ì§€ ë³€ê²½...")
            realign_action = menu.addAction("ì—¬ê¸°ì„œ ë‹¤ì‹œ ì •ë ¬")
            menu.addSeparator()
            delete_action = menu.addAction("ì‚­ì œ")
            
            action = menu.exec(pos)
            
            if action == change_image_action:
                self._change_clip_image(clip)
            elif action == realign_action:
                self._realign_images_from(clip)
            elif action == delete_action:
                self._delete_clip(clip)
        
        elif clip.clip_type == "audio":
            insert_image_action = menu.addAction("ì´ ìœ„ì¹˜ì— ì´ë¯¸ì§€ ì‚½ì…...")
            menu.addSeparator()
            
            action = menu.exec(pos)
            
            if action == insert_image_action:
                self._insert_image_at_clip(clip)
    
    def _find_adjacent_subtitle(self, clip, direction=1):
        """Find adjacent subtitle clip (direction: 1=next, -1=prev)"""
        sub_clips = [c for c in self.timeline_widget.canvas.clips 
                     if c.clip_type == "subtitle"]
        sub_clips.sort(key=lambda c: c.start)
        
        try:
            idx = next(i for i, c in enumerate(sub_clips) if c.id == clip.id)
            target_idx = idx + direction
            if 0 <= target_idx < len(sub_clips):
                return sub_clips[target_idx]
        except StopIteration:
            pass
        return None
    
    def _show_subtitle_editor(self, clip):
        """Show dialog for splitting subtitle at a specific point"""
        from PyQt6.QtWidgets import QDialog, QVBoxLayout, QLabel, QTextEdit, QPushButton, QHBoxLayout
        
        dialog = QDialog(self)
        dialog.setWindowTitle("ìë§‰ ë‚˜ëˆ„ê¸°")
        dialog.setMinimumWidth(400)
        
        layout = QVBoxLayout(dialog)
        layout.addWidget(QLabel("ë‚˜ëˆŒ ìœ„ì¹˜ì˜ ì• í…ìŠ¤íŠ¸ë¥¼ ë‚¨ê¸°ì„¸ìš”:"))
        
        text_edit = QTextEdit()
        text_edit.setPlainText(clip.name)
        layout.addWidget(text_edit)
        
        layout.addWidget(QLabel("ì»¤ì„œ ìœ„ì¹˜ì—ì„œ ìë§‰ì´ ë‚˜ëˆ ì§‘ë‹ˆë‹¤."))
        
        btn_layout = QHBoxLayout()
        split_btn = QPushButton("ë‚˜ëˆ„ê¸°")
        cancel_btn = QPushButton("ì·¨ì†Œ")
        btn_layout.addWidget(split_btn)
        btn_layout.addWidget(cancel_btn)
        layout.addLayout(btn_layout)
        
        def do_split():
            cursor_pos = text_edit.textCursor().position()
            self._split_subtitle_at(clip, cursor_pos)
            dialog.accept()
        
        split_btn.clicked.connect(do_split)
        cancel_btn.clicked.connect(dialog.reject)
        
        dialog.exec()
    
    def _split_subtitle_at(self, clip, char_pos: int):
        """Split subtitle clip at character position (NEW API)"""
        from core.subtitle_processor import SubtitleProcessor
        
        if char_pos <= 0 or char_pos >= len(clip.name):
            self.statusBar().showMessage("ë‚˜ëˆŒ ìœ„ì¹˜ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        # Audio-anchored split only
        if not clip.words:
            self.statusBar().showMessage("ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ì–´ ì˜¤ë””ì˜¤ ê¸°ì¤€ ë¶„í• ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        audio_anchor = self._find_linked_audio_clip_for_subtitle(clip)
        if not audio_anchor:
            self.statusBar().showMessage("ì—°ê²°ëœ ì˜¤ë””ì˜¤ í´ë¦½ì„ ì°¾ì§€ ëª»í•´ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        processor = SubtitleProcessor(lead_time_ms=self.runtime_config.subtitle_lead_time_ms)
        
        # NEW API: í†µí•© Fuzzy Matchingìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
        split_indices = [char_pos]
        timestamps = processor.calculate_split_times(clip.name, split_indices, clip.words)
        
        if not timestamps:
            self.statusBar().showMessage("ë¶„í•  íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        source_split_time = timestamps[0]
        split_time = audio_anchor.start + (source_split_time - audio_anchor.offset)

        # Final guard: do not allow splits outside the current clip span
        clip_start = clip.start
        clip_end = clip.start + clip.duration
        if split_time <= clip_start + 0.05 or split_time >= clip_end - 0.05:
            self.statusBar().showMessage(
                "ë¶„í•  ìœ„ì¹˜ê°€ í˜„ì¬ ìë§‰ í´ë¦½ ë²”ìœ„ ë°–ì…ë‹ˆë‹¤. ìë§‰ í´ë¦½ì„ ë¨¼ì € ì´ë™/í™•ì¥í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
            )
            return
        
        # Use cursor position directly for text split (user intent)
        actual_split_pos = char_pos
        
        # Skip trailing spaces
        while actual_split_pos < len(clip.name) and clip.name[actual_split_pos] == ' ':
            actual_split_pos += 1
        
        # Create two text segments
        text1 = clip.name[:actual_split_pos].strip()
        text2 = clip.name[actual_split_pos:].strip()
        
        # Split words based on fuzzy-matched timestamp
        words1 = []
        words2 = []
        for w in clip.words:
            w_end = w.end if hasattr(w, 'end') else 0.0
            if w_end <= source_split_time:
                words1.append(w)
            else:
                words2.append(w)
        
        # Save original end time before modifying
        original_end = clip.start + clip.duration
        
        old_clip_state = copy.deepcopy(clip)

        # Update original clip (first segment ends at split_time)
        clip.name = text1
        clip.duration = split_time - clip.start
        clip.words = words1
        
        new_clip_state = copy.deepcopy(clip)

        # Create new clip (second segment starts at split_time)
        new_id = self._make_unique_clip_id(f"{clip.id}_split")
        new_duration = original_end - split_time
        
        new_clip = TimelineClip(
            id=new_id,
            name=text2,
            start=split_time,
            duration=new_duration,
            track=clip.track,
            color=clip.color,
            clip_type="subtitle",
            waveform=[],
            offset=source_split_time,
            segment_index=clip.segment_index,
            speaker=clip.speaker,
            words=words2
        )
        
        # Create Undo Commands
        modify_cmd = ModifyClipsCommand(
            self.timeline_widget.canvas,
            [(clip.id, old_clip_state, new_clip_state)],
            description="Split subtitle (modify original)"
        )

        add_cmd = AddRemoveClipsCommand(
            self.timeline_widget.canvas,
            added=[new_clip],
            removed=[],
            description="Split subtitle (add new)"
        )

        # Execute actions manually first since we already modified 'clip' in place above,
        # but we haven't added 'new_clip' yet.
        self.timeline_widget.canvas.clips.append(new_clip)

        # Push composite command
        macro_cmd = MacroCommand([modify_cmd, add_cmd], description="Split subtitle", callback=self._on_undo_redo_callback)
        self.undo_stack.push(macro_cmd)
        self._update_undo_redo_actions()

        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        self.statusBar().showMessage("ìë§‰ì´ ë‚˜ëˆ ì¡ŒìŠµë‹ˆë‹¤.")

    def _find_linked_audio_clip_for_subtitle(self, subtitle_clip):
        """Find the most likely linked audio clip for a subtitle clip.

        Uses segment_index as primary key; falls back to ID conventions.
        """
        clips = getattr(self.timeline_widget.canvas, 'clips', [])
        audio_clips = [c for c in clips if getattr(c, 'clip_type', None) == 'audio']
        if not audio_clips:
            return None

        # Strong match: same segment_index
        if getattr(subtitle_clip, 'segment_index', -1) is not None and subtitle_clip.segment_index >= 0:
            candidates = [
                c for c in audio_clips
                if getattr(c, 'segment_index', -1) == subtitle_clip.segment_index
                and (not getattr(subtitle_clip, 'speaker', '') or getattr(c, 'speaker', '') == subtitle_clip.speaker)
            ]
            if candidates:
                return min(candidates, key=lambda c: abs(c.start - subtitle_clip.start))

        # Fallback: sub_{i} <-> audio_{i}
        try:
            if isinstance(subtitle_clip.id, str) and subtitle_clip.id.startswith('sub_'):
                base = subtitle_clip.id.split('_', 2)[1]
                expected_audio_id = f"audio_{base}"
                for c in audio_clips:
                    if c.id == expected_audio_id:
                        return c
        except Exception:
            pass

        return None

    def _is_audio_anchor_usable(self, subtitle_clip, audio_clip) -> bool:
        """Heuristic: determine whether audio anchor is reliable enough.

        We intentionally allow subtitle clips to be source-trimmed (offset changes)
        and/or moved on the timeline; as long as we have a plausible linked audio
        clip, we can anchor split boundaries to audio.

        We only fall back when the subtitle and audio are *too* far apart on the
        timeline (likely different content / bad link).
        """
        if not subtitle_clip or not audio_clip:
            return False

        sub_start = getattr(subtitle_clip, 'start', 0.0)
        sub_end = sub_start + getattr(subtitle_clip, 'duration', 0.0)
        aud_start = getattr(audio_clip, 'start', 0.0)
        aud_end = aud_start + getattr(audio_clip, 'duration', 0.0)

        # Too far apart (timeline desync)
        if max(abs(sub_start - aud_start), abs(sub_end - aud_end)) > 2.0:
            return False

        return True
    
    def _merge_subtitle_clips(self, clip1, clip2):
        """Merge two adjacent subtitle clips"""
        from core.subtitle_processor import SubtitleProcessor

        if not clip1 or not clip2:
            return

        # Only allow merges that preserve a single audio anchor.
        # Merging across different segments or with timeline/source gaps produces
        # a subtitle clip that cannot be reliably split/anchored to audio later.
        if getattr(clip1, 'track', None) != getattr(clip2, 'track', None):
            self.statusBar().showMessage("ê°™ì€ íŠ¸ë™ì˜ ìë§‰ë§Œ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return

        clip1_end = getattr(clip1, 'start', 0.0) + getattr(clip1, 'duration', 0.0)
        clip2_start = getattr(clip2, 'start', 0.0)
        if abs(clip1_end - clip2_start) > 0.1:
            self.statusBar().showMessage("ì„œë¡œ ì¸ì ‘í•œ ìë§‰ë§Œ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return

        if getattr(clip1, 'segment_index', -1) != getattr(clip2, 'segment_index', -1):
            self.statusBar().showMessage("ì„œë¡œ ë‹¤ë¥¸ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì˜ ìë§‰ì€ ë³‘í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        if getattr(clip1, 'speaker', '') != getattr(clip2, 'speaker', ''):
            self.statusBar().showMessage("ì„œë¡œ ë‹¤ë¥¸ í™”ìì˜ ìë§‰ì€ ë³‘í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # Source continuity guard (offset-based). Allow tiny jitter.
        clip1_source_end = getattr(clip1, 'offset', 0.0) + getattr(clip1, 'duration', 0.0)
        clip2_source_start = getattr(clip2, 'offset', 0.0)
        if abs(clip1_source_end - clip2_source_start) > 0.2:
            self.statusBar().showMessage("ì›ë³¸ ì˜¤ë””ì˜¤ ë²”ìœ„ê°€ ì—°ì†ì ì´ì§€ ì•Šì•„ ë³‘í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        a1 = self._find_linked_audio_clip_for_subtitle(clip1)
        a2 = self._find_linked_audio_clip_for_subtitle(clip2)
        if not a1 or not a2 or getattr(a1, 'id', None) != getattr(a2, 'id', None):
            self.statusBar().showMessage("ì—°ê²°ëœ ì˜¤ë””ì˜¤ê°€ ë‹¬ë¼ ë³‘í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        processor = SubtitleProcessor(lead_time_ms=self.runtime_config.subtitle_lead_time_ms)
        # Use source audio coordinates for merge_segments
        # (though it doesn't do coordinate transformation, using offset maintains consistency)
        merged = processor.merge_segments(
            {'text': clip1.name, 'start_time': clip1.offset, 
             'end_time': clip1.offset + clip1.duration, 'words': clip1.words},
            {'text': clip2.name, 'start_time': clip2.offset,
             'end_time': clip2.offset + clip2.duration, 'words': clip2.words}
        )
        
        old_clip1_state = copy.deepcopy(clip1)

        # Update first clip - keep timeline start, update duration, preserve offset
        clip1.name = merged['text']
        # Calculate new timeline duration: from clip1.start to clip2's end
        new_timeline_end = clip2.start + clip2.duration
        clip1.duration = new_timeline_end - clip1.start
        clip1.words = merged['words']
        # clip1.offset stays the same (it's the original start time)
        
        new_clip1_state = copy.deepcopy(clip1)

        # Remove second clip
        self.timeline_widget.canvas.clips.remove(clip2)

        # Undo Commands
        modify_cmd = ModifyClipsCommand(
            self.timeline_widget.canvas,
            [(clip1.id, old_clip1_state, new_clip1_state)],
            description="Merge subtitles (modify first)"
        )

        remove_cmd = AddRemoveClipsCommand(
            self.timeline_widget.canvas,
            added=[],
            removed=[clip2],
            description="Merge subtitles (remove second)"
        )

        macro_cmd = MacroCommand([modify_cmd, remove_cmd], description="Merge subtitles", callback=self._on_undo_redo_callback)
        self.undo_stack.push(macro_cmd)
        self._update_undo_redo_actions()

        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        self.statusBar().showMessage("ìë§‰ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _auto_format_subtitles(self):
        """Apply automatic formatting to all subtitle clips (NEW API)"""
        from core.subtitle_processor import SubtitleProcessor
        
        # Use runtime config for subtitle settings
        config = self.runtime_config
        
        # Collect subtitle clips
        subtitle_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "subtitle"]
        
        if not subtitle_clips:
            self.statusBar().showMessage("ìë§‰ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Create processors for each language (cached)
        processor_cache = {}
        
        def get_processor(language: str) -> SubtitleProcessor:
            if language not in processor_cache:
                if config.subtitle_auto_params:
                    params = config.get_subtitle_params(language)
                else:
                    params = {
                        'line_soft_cap': config.subtitle_line_soft_cap,
                        'line_hard_cap': config.subtitle_line_hard_cap,
                        'max_lines': config.subtitle_max_lines,
                    }
                processor_cache[language] = SubtitleProcessor(
                    line_soft_cap=params['line_soft_cap'],
                    line_hard_cap=params['line_hard_cap'],
                    max_lines=params['max_lines'],
                    split_on_conjunctions=config.subtitle_split_on_conjunctions,
                    lead_time_ms=config.subtitle_lead_time_ms
                )
            return processor_cache[language]
        
        new_clips = []
        existing_non_subtitle_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type != "subtitle"]
        used_ids = {c.id for c in existing_non_subtitle_clips}

        def reserve_id(base_id: str) -> str:
            """Reserve and return a unique clip id for this rebuild pass."""
            candidate = base_id
            suffix = 1
            while candidate in used_ids:
                candidate = f"{base_id}_{suffix}"
                suffix += 1
            used_ids.add(candidate)
            return candidate
        
        split_count = 0
        format_count = 0
        
        for clip in subtitle_clips:
            original_text = clip.name
            audio_anchor = self._find_linked_audio_clip_for_subtitle(clip)

            base_id = (
                f"subseg_{clip.segment_index}" if getattr(clip, 'segment_index', -1) is not None and clip.segment_index >= 0
                else "subseg"
            )
            
            # Detect language and get appropriate processor
            if '_detector' not in processor_cache:
                processor_cache['_detector'] = SubtitleProcessor(lead_time_ms=config.subtitle_lead_time_ms)
            language = processor_cache['_detector'].detect_language(original_text)
            processor = get_processor(language)
            
            # NEW API: 1. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„í•  í¬ì¸íŠ¸ ì°¾ê¸°
            segment_splits = processor.find_split_points(original_text, is_segment=True)
            
            if not segment_splits or not clip.words or not audio_anchor:
                # ë¶„í•  ë¶ˆí•„ìš” ë˜ëŠ” ë¶ˆê°€ëŠ¥ - ì¤„ë°”ê¿ˆë§Œ ì ìš©
                line_breaks = processor.find_split_points(original_text, is_segment=False)
                
                if line_breaks:
                    # ì¤„ë°”ê¿ˆ ì ìš©
                    lines = []
                    prev = 0
                    for lb in line_breaks:
                        lines.append(original_text[prev:lb].strip())
                        prev = lb + 1
                    lines.append(original_text[prev:].strip())
                    formatted_text = '\n'.join(lines)
                else:
                    formatted_text = original_text
                
                new_clips.append(TimelineClip(
                    id=reserve_id(f"{base_id}_0"),
                    name=formatted_text,
                    start=clip.start,
                    duration=clip.duration,
                    track=clip.track,
                    color=clip.color,
                    clip_type="subtitle",
                    waveform=[],
                    offset=clip.offset,
                    segment_index=clip.segment_index,
                    speaker=clip.speaker,
                    words=clip.words,
                ))
                
                if formatted_text != original_text:
                    format_count += 1
                continue

            # NEW API: 2. Fuzzy Matchingìœ¼ë¡œ ë¶„í•  íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
            split_timestamps = processor.calculate_split_times(original_text, segment_splits, clip.words)
            
            if not split_timestamps or len(split_timestamps) != len(segment_splits):
                # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚° ì‹¤íŒ¨ - ì¤„ë°”ê¿ˆë§Œ ì ìš©
                line_breaks = processor.find_split_points(original_text, is_segment=False)
                formatted_text = original_text
                if line_breaks:
                    lines = []
                    prev = 0
                    for lb in line_breaks:
                        lines.append(original_text[prev:lb].strip())
                        prev = lb + (1 if original_text[lb] == ' ' else 0)
                    lines.append(original_text[prev:].strip())
                    formatted_text = '\n'.join(lines)
                
                new_clips.append(TimelineClip(
                    id=reserve_id(f"{base_id}_0"),
                    name=formatted_text,
                    start=clip.start,
                    duration=clip.duration,
                    track=clip.track,
                    color=clip.color,
                    clip_type="subtitle",
                    waveform=[],
                    offset=clip.offset,
                    segment_index=clip.segment_index,
                    speaker=clip.speaker,
                    words=clip.words,
                ))
                if formatted_text != original_text:
                    format_count += 1
                continue

            # 3. íƒ€ì„ë¼ì¸ ì¢Œí‘œë¡œ ë³€í™˜ (ì˜¤ë””ì˜¤ ì•µì»¤ ê¸°ì¤€)
            boundaries_timeline = []
            for source_time in split_timestamps:
                timeline_time = audio_anchor.start + (source_time - audio_anchor.offset)
                boundaries_timeline.append(timeline_time)
            
            # Validate boundaries are within the current subtitle clip range
            clip_start = clip.start
            clip_end = clip.start + clip.duration
            valid_boundaries = True
            for b in boundaries_timeline:
                if b <= clip_start + 0.05 or b >= clip_end - 0.05:
                    valid_boundaries = False
                    break
            
            if not valid_boundaries:
                # ê²½ê³„ê°€ í´ë¦½ ë²”ìœ„ ë°– - ë¶„í•  ë¶ˆê°€ëŠ¥, ì¤„ë°”ê¿ˆë§Œ ì ìš©
                line_breaks = processor.find_split_points(original_text, is_segment=False)
                formatted_text = original_text
                if line_breaks:
                    lines = []
                    prev = 0
                    for lb in line_breaks:
                        lines.append(original_text[prev:lb].strip())
                        prev = lb + (1 if original_text[lb] == ' ' else 0)
                    lines.append(original_text[prev:].strip())
                    formatted_text = '\n'.join(lines)
                
                new_clips.append(TimelineClip(
                    id=reserve_id(f"{base_id}_0"),
                    name=formatted_text,
                    start=clip.start,
                    duration=clip.duration,
                    track=clip.track,
                    color=clip.color,
                    clip_type="subtitle",
                    waveform=[],
                    offset=clip.offset,
                    segment_index=clip.segment_index,
                    speaker=clip.speaker,
                    words=clip.words,
                ))
                if formatted_text != original_text:
                    format_count += 1
                continue

            # 4. í…ìŠ¤íŠ¸ì™€ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            segments_text = []
            segments_time = []
            segments_words = []
            
            prev_idx = 0
            prev_time = clip.start
            prev_source_time = clip.offset
            
            for split_idx, split_source_time, split_timeline_time in zip(
                segment_splits, split_timestamps, boundaries_timeline
            ):
                seg_text = original_text[prev_idx:split_idx].strip()
                segments_text.append(seg_text)
                segments_time.append((prev_time, split_timeline_time))
                
                # ë‹¨ì–´ ë¶„í• 
                seg_words = [w for w in clip.words 
                            if hasattr(w, 'start') and hasattr(w, 'end') 
                            and w.start >= prev_source_time and w.end <= split_source_time]
                segments_words.append(seg_words)
                
                prev_idx = split_idx
                prev_time = split_timeline_time
                prev_source_time = split_source_time
            
            # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸
            segments_text.append(original_text[prev_idx:].strip())
            segments_time.append((prev_time, clip_end))
            seg_words = [w for w in clip.words 
                        if hasattr(w, 'start') and w.start >= prev_source_time]
            segments_words.append(seg_words)
            
            # 5. ê° ì„¸ê·¸ë¨¼íŠ¸ì— ì¤„ë°”ê¿ˆ ì ìš© ë° í´ë¦½ ìƒì„±
            for i, (seg_text, (seg_start, seg_end), seg_words) in enumerate(
                zip(segments_text, segments_time, segments_words)
            ):
                # ì¤„ë°”ê¿ˆ ì²˜ë¦¬
                line_breaks = processor.find_split_points(seg_text, is_segment=False)
                
                if line_breaks:
                    lines = []
                    prev = 0
                    for lb in line_breaks:
                        lines.append(seg_text[prev:lb].strip())
                        prev = lb + (1 if seg_text[lb] == ' ' else 0)
                    lines.append(seg_text[prev:].strip())
                    formatted_text = '\n'.join(lines)
                else:
                    formatted_text = seg_text
                
                timeline_duration = seg_end - seg_start
                
                if timeline_duration <= 0.05:
                    # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ê±´ë„ˆë›°ê¸°
                    continue
                
                # offsetì€ source audio ì¢Œí‘œ
                segment_offset = clip.offset if i == 0 else split_timestamps[i-1]
                
                new_clip = TimelineClip(
                    id=reserve_id(f"{base_id}_{i}"),
                    name=formatted_text,
                    start=seg_start,
                    duration=timeline_duration,
                    track=clip.track,
                    color=clip.color,
                    clip_type="subtitle",
                    waveform=[],
                    offset=segment_offset,
                    segment_index=clip.segment_index,
                    speaker=clip.speaker,
                    words=seg_words
                )
                new_clips.append(new_clip)
            
            if len(segments_text) > 1:
                split_count += 1
            if any('\n' in seg for seg in segments_text):
                format_count += 1
        
        # Keep old clips for undo
        old_clips_list = copy.deepcopy(self.timeline_widget.canvas.clips)

        # Replace subtitle clips
        new_clips_list = existing_non_subtitle_clips + new_clips
        self.timeline_widget.canvas.clips = new_clips_list

        # Undo Command
        cmd = ReplaceAllClipsCommand(
            self.timeline_widget.canvas,
            old_clips=old_clips_list,
            new_clips=copy.deepcopy(new_clips_list),
            description="Auto format subtitles",
            callback=self._on_undo_redo_callback
        )
        self.undo_stack.push(cmd)
        self._update_undo_redo_actions()

        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        
        self.statusBar().showMessage(f"ìë§‰ ì •ë¦¬ ì™„ë£Œ: {split_count}ê°œ ë¶„í• , {format_count}ê°œ ì¤„ë°”ê¿ˆ ì ìš©")
    
    def _realign_images_from(self, start_clip):
        """Realign image clips to match audio clips starting from the given image
        
        This takes all images from start_clip onwards and aligns them 1:1 with
        audio clips that start at or after start_clip's position.
        """
        # Get all image clips sorted by start time
        all_image_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "image"]
        all_image_clips.sort(key=lambda c: c.start)
        
        # Find index of start_clip
        try:
            start_idx = next(i for i, c in enumerate(all_image_clips) if c.id == start_clip.id)
        except StopIteration:
            return
        
        # Get images to realign (from start_clip onwards)
        images_to_realign = all_image_clips[start_idx:]
        
        # Get audio clips sorted by start time
        audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
        audio_clips.sort(key=lambda c: c.start)
        
        # Find the audio clip that contains the image's current position
        # (the audio clip where start <= image_pos < start + duration)
        current_pos = start_clip.start
        start_audio_idx = 0
        
        for i, audio in enumerate(audio_clips):
            if audio.start <= current_pos < audio.start + audio.duration:
                start_audio_idx = i
                break
            elif audio.start > current_pos:
                # Image is before any audio clip, use the first available
                start_audio_idx = i
                break
        
        available_audio = audio_clips[start_audio_idx:]
        
        if not available_audio:
            self.statusBar().showMessage("ì •ë ¬í•  ì˜¤ë””ì˜¤ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Realign: each image matches an audio clip
        realigned_count = 0
        modifications = []

        for i, img_clip in enumerate(images_to_realign):
            if i >= len(available_audio):
                break
            
            audio_clip = available_audio[i]
            
            # Find next audio clip's start for duration calculation
            if i + 1 < len(available_audio):
                next_audio_start = available_audio[i + 1].start
            else:
                next_audio_start = audio_clip.start + audio_clip.duration
            
            # Record state
            old_state = copy.deepcopy(img_clip)

            # Update image clip
            img_clip.start = audio_clip.start
            img_clip.duration = next_audio_start - audio_clip.start

            new_state = copy.deepcopy(img_clip)
            modifications.append((img_clip.id, old_state, new_state))

            realigned_count += 1
        
        if modifications:
            cmd = ModifyClipsCommand(
                self.timeline_widget.canvas,
                modifications,
                description="Realign images",
                callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(cmd)
            self._update_undo_redo_actions()

        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
        self.statusBar().showMessage(f"ì´ë¯¸ì§€ {realigned_count}ê°œê°€ ë‹¤ì‹œ ì •ë ¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _change_clip_image(self, clip):
        """Change the image of an image clip"""
        # Check if there's a selected image in the list
        selected_items = self.image_list.selectedItems()
        path = None
        
        if selected_items:
            # Use selected image from list
            selected_path = selected_items[0].data(Qt.ItemDataRole.UserRole)
            if selected_path:
                path = selected_path
        
        if not path:
            # Otherwise, open file dialog
            path_sel, _ = QFileDialog.getOpenFileName(
                self, "ì´ë¯¸ì§€ ì„ íƒ", "",
                "Image Files (*.png *.jpg *.jpeg *.webp);;All Files (*)"
            )
            if path_sel:
                path = path_sel

        if path:
            old_state = copy.deepcopy(clip)
            clip.image_path = path
            clip.name = Path(path).name
            new_state = copy.deepcopy(clip)

            # Undo command
            cmd = ModifyClipsCommand(
                self.timeline_widget.canvas,
                [(clip.id, old_state, new_state)],
                description=f"Change image to {clip.name}",
                callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(cmd)
            self._update_undo_redo_actions()

            self.timeline_widget.canvas._background_dirty = True
            self.timeline_widget.canvas.update()
            
            # Load new image into cache (prefetch original for preview)
            from .image_cache import get_image_cache
            cache = get_image_cache()
            if not cache.is_loaded(path):
                cache.prefetch_images([path])
            
            
            playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
            self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤: {clip.name}")
    
    def _insert_image_at_clip(self, audio_clip):
        """Insert a new image clip at the position of an audio clip (ìŠ¤ë§ˆíŠ¸ ì‚½ì… v3)"""
        
        # Check if there's a selected image in the list
        selected_items = self.image_list.selectedItems()
        image_path = None
        
        if selected_items:
            image_path = selected_items[0].data(Qt.ItemDataRole.UserRole)
        
        if not image_path:
            # Open file dialog
            path, _ = QFileDialog.getOpenFileName(
                self, "ì´ë¯¸ì§€ ì„ íƒ", "",
                "Image Files (*.png *.jpg *.jpeg *.webp);;All Files (*)"
            )
            if path:
                image_path = path
        
        if not image_path:
            return
        
        # t = ì˜¤ë””ì˜¤ í´ë¦½ ì‹œì‘ ì‹œê°„
        t = audio_clip.start
        EPSILON = 1e-4
        
        # ì´ë¯¸ì§€ í´ë¦½ë“¤ ìˆ˜ì§‘
        image_clips = sorted(
            [c for c in self.timeline_widget.canvas.clips if c.clip_type == "image"],
            key=lambda c: c.start
        )
        
        # ì˜¤ë””ì˜¤ í´ë¦½ë“¤ ìˆ˜ì§‘ (duration ê³„ì‚°ìš©)
        audio_clips = sorted(
            [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"],
            key=lambda c: c.start
        )
        
        modifications = []
        
        # 1. ì•ì— ê¸´ í´ë¦½ì´ të¥¼ í¬í•¨í•˜ëŠ” ê²½ìš° => ì• í´ë¦½ ëì ì„ të¡œ ìë¦„
        # (tê°€ startë³´ë‹¤ í™•ì‹¤íˆ ì»¤ì•¼ í•¨)
        for clip in image_clips:
            if clip.start < t - EPSILON and t < clip.start + clip.duration - EPSILON:
                old_state = copy.deepcopy(clip)
                clip.duration = t - clip.start
                new_state = copy.deepcopy(clip)
                modifications.append((clip.id, old_state, new_state))
                break
        
        # 2. ëì  ê²°ì • ì•Œê³ ë¦¬ì¦˜
        # ê¸°ë³¸ ëì : ë‹¤ìŒ ì˜¤ë””ì˜¤ í´ë¦½ ì‹œì‘ì 
        next_audio_start = float('inf')
        for ac in audio_clips:
            if ac.start > t + EPSILON:
                 next_audio_start = ac.start
                 break
        
        if next_audio_start == float('inf'):
             # ë§ˆì§€ë§‰ ì˜¤ë””ì˜¤ í´ë¦½ì¸ ê²½ìš°: ì˜¤ë””ì˜¤ í´ë¦½ì˜ ëì ì„ ëª©í‘œë¡œ í•¨
             next_audio_start = audio_clip.start + audio_clip.duration
             
        end_time = next_audio_start
        
        # ë’¤ì˜ ì´ë¯¸ì§€ í´ë¦½ë“¤ê³¼ì˜ ê°„ì„­ ê³ ë ¤
        # tì—ì„œ 1ì´ˆ ì•ˆì— ì‹œì‘í•˜ëŠ” í´ë¦½ -> (ê·¸ í´ë¦½ ëì  + t) / 2
        # 1ì´ˆ ë°–ì—ì„œ ì‹œì‘í•˜ëŠ” í´ë¦½ -> ê·¸ í´ë¦½ ì‹œì‘ì 
        # ì´ ì ë“¤ì˜ ìµœì†Ÿê°’ì„ ëì ìœ¼ë¡œ ë‘”ë‹¤.
        
        # ì£¼ì˜: ì‹œì‘ ì‹œê°„ì´ tì™€ ê°™ì€ í´ë¦½ë„ í¬í•¨í•´ì•¼ í•¨ (>= t - EPSILON)
        for clip in image_clips:
            if clip.start > t - EPSILON: # t ì´í›„(ë˜ëŠ” ê°™ì€) ì‹œì‘
                proposed_end = clip.start # ê¸°ë³¸: ì‹œì‘ì 
                
                # 1ì´ˆ ì´ë‚´ ì‹œì‘ (ì‹œì‘ ì‹œê°„ ì¼ì¹˜ í¬í•¨)
                if clip.start - t <= 1.0 + EPSILON: 
                    proposed_end = (clip.start + clip.duration + t) / 2
                
                if proposed_end < end_time:
                    end_time = proposed_end
        
        # 3. ë’¤ì˜ í´ë¦½ë“¤ ê²¹ì¹˜ëŠ” ê±° ì‹œì‘ì ì„ ì´ ëíŒì™•(end_time)ìœ¼ë¡œ ë‹¤ ë°”ê¿ˆ
        for clip in image_clips:
            # t ì´í›„(ë˜ëŠ” ê°™ì€) ì‹œì‘í•˜ê³ , end_timeë³´ë‹¤ ë¨¼ì € ì‹œì‘í•˜ëŠ” ê²ƒë“¤
            if clip.start > t - EPSILON and clip.start < end_time - EPSILON:
                 old_state = copy.deepcopy(clip)
                 
                 # ì‹œì‘ì  ì´ë™ (ëì ì€ ìœ ì§€ = duration ê°ì†Œ)
                 original_end = clip.start + clip.duration
                 clip.start = end_time
                 clip.duration = max(0.1, original_end - end_time) # ìµœì†Œ 0.1s ë³´í˜¸
                 
                 new_state = copy.deepcopy(clip)
                 modifications.append((clip.id, old_state, new_state))

        # Generate unique ID
        new_id = self._make_unique_clip_id(f"img_inserted_{len(audio_clips)}")
        
        # Create new image clip
        new_clip = TimelineClip(
            id=new_id,
            name=Path(image_path).name,
            start=t,
            duration=end_time - t, # ê³„ì‚°ëœ ëì  ì‚¬ìš©
            track=2,
            color=QColor("#9E9E9E"),
            clip_type="image",
            waveform=[],
            image_path=image_path
        )
        
        # Undo commands (MacroCommand)
        commands = []
        
        if modifications:
            modify_cmd = ModifyClipsCommand(
                self.timeline_widget.canvas,
                modifications,
                description="ê¸°ì¡´ í´ë¦½ ê²½ê³„ ì¡°ì •",
                callback=None
            )
            commands.append(modify_cmd)
        
        add_cmd = AddRemoveClipsCommand(
            self.timeline_widget.canvas,
            added=[new_clip],
            removed=[],
            description=f"Insert image {new_clip.name}",
            callback=None # MacroCommand handles callback
        )
        commands.append(add_cmd)
        
        if len(commands) > 1:
            macro_cmd = MacroCommand(
                 commands,
                 description=f"ìŠ¤ë§ˆíŠ¸ ì‚½ì…: {new_clip.name}",
                 callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(macro_cmd)
            macro_cmd.redo()
        else:
            # If only one command, make sure it has the callback
            add_cmd.callback = self._on_undo_redo_callback
            self.undo_stack.push(add_cmd)
            add_cmd.redo()

        self.timeline_widget.canvas._update_total_duration()
        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        
        # Load image into cache for thumbnail display and prefetch for preview
        from .image_cache import get_image_cache
        cache = get_image_cache()
        if not cache.is_loaded(image_path):
            cache.prefetch_images([image_path])
        
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        self.statusBar().showMessage(f"ì´ë¯¸ì§€ê°€ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤: {new_clip.name}")
    
    def _calculate_smart_image_position(self, drop_time: float, margin: float = 0.5):
        """
        ìŠ¤ë§ˆíŠ¸ ì´ë¯¸ì§€ ì‚½ì… ìœ„ì¹˜ì™€ duration ê³„ì‚° (m, M ì•Œê³ ë¦¬ì¦˜)
        
        Args:
            drop_time: ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë“œë¡­ ì‹œê°„ (t)
            margin: í´ë¦½ ê°„ ìµœì†Œ ê°„ê²© (ê¸°ë³¸ 0.5ì´ˆ, ê° í´ë¦½ ìµœì†Œ 0.25ì´ˆ ë³´ì¥)
            
        Returns:
            tuple: (adjusted_start, duration, clips_to_modify)
        """
        EPSILON = 1e-4
        
        # ì´ë¯¸ì§€ í´ë¦½ë“¤ì˜ ì‹œì‘ ì‹œê°„ ìˆ˜ì§‘
        image_clips = sorted(
            [c for c in self.timeline_widget.canvas.clips if c.clip_type == "image"],
            key=lambda c: c.start
        )
        image_starts = [c.start for c in image_clips]
        
        # ì˜¤ë””ì˜¤ í´ë¦½ë“¤ (duration ê³„ì‚°ìš©)
        audio_clips = sorted(
            [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"],
            key=lambda c: c.start
        )
        
        t = drop_time
        
        # m = max(si + margin) for si < t (ì• í´ë¦½ë“¤ì´ í—ˆìš©í•˜ëŠ” ìµœì†Œ ì‹œì‘ì )
        # M = min(si - margin) for si > t (ë’¤ í´ë¦½ë“¤ì´ í—ˆìš©í•˜ëŠ” ìµœëŒ€ ì‹œì‘ì )
        # EPSILONì„ ì‚¬ìš©í•˜ì—¬ ê²½ê³„ê°’ ì²˜ë¦¬ ê°•í™”
        m = max((si + margin for si in image_starts if si < t - EPSILON), default=0.0)
        M = min((si - margin for si in image_starts if si > t + EPSILON), default=float('inf'))
        
        # ìœ„ì¹˜ ê²°ì •
        if m <= t + EPSILON and t <= M + EPSILON:
            # ì¶©ë¶„í•œ ê³µê°„ - ì›í•˜ëŠ” ìœ„ì¹˜ ê·¸ëŒ€ë¡œ
            adjusted_start = t
        elif m <= M < t: # ì—¬ìœ  ê³µê°„ì´ ìˆì§€ë§Œ tê°€ ë’¤ì— ìˆìŒ -> Mìœ¼ë¡œ ë‹¹ê¹€
            # ë’¤ìª½ì— ë§‰í˜ - Mìœ¼ë¡œ ë‹¹ê¹€
            adjusted_start = M
        elif t < m <= M: # ì—¬ìœ  ê³µê°„ì´ ìˆì§€ë§Œ tê°€ ì•ì— ìˆìŒ -> mìœ¼ë¡œ ë°€ë¦¼
            # ì•ìª½ì— ë§‰í˜ - mìœ¼ë¡œ ë°€ë¦¼
            adjusted_start = m
        else:  # m > M (ì™„ì „íˆ ë¼ì¸ ê²½ìš° - start + margin > next_start - margin)
            # ì¤‘ì•™ê°’ìœ¼ë¡œ ì‚½ì… (ì–‘ìª½ í´ë¦½ ê°ê° ìµœì†Œ margin/2 = 0.25ì´ˆ í™•ë³´)
            adjusted_start = (m + M) / 2
        
        # ìŒìˆ˜ ë°©ì§€
        adjusted_start = max(0.0, adjusted_start)
        
        # ê²¹ì¹˜ëŠ” ì• í´ë¦½ ì°¾ê¸° (ê²½ê³„ ì¡°ì • í•„ìš”)
        clips_to_modify = []
        for clip in image_clips:
            # adjusted_startê°€ í´ë¦½ ë²”ìœ„ ì•ˆì— ìˆìœ¼ë©´ ìë¥´ê¸° (start < adj < end)
            # ì •í™•íˆëŠ” adjê°€ startë³´ë‹¤ ì»¤ì•¼ í•¨.
            if clip.start < adjusted_start - EPSILON and adjusted_start < clip.start + clip.duration - EPSILON:
                clips_to_modify.append((clip, adjusted_start))
                break
        
        # Duration ê³„ì‚°: ë‹¤ìŒ ì´ë¯¸ì§€ í´ë¦½ ë˜ëŠ” ë‹¤ìŒ ì˜¤ë””ì˜¤ í´ë¦½ê¹Œì§€
        next_image_start = min((si for si in image_starts if si > adjusted_start + EPSILON), default=float('inf'))
        next_audio_start = min((c.start for c in audio_clips if c.start > adjusted_start + EPSILON), default=float('inf'))
        end_limit = min(next_image_start, next_audio_start)
        
        if end_limit == float('inf'):
            # ë‹¤ìŒ í´ë¦½ì´ ì—†ìœ¼ë©´ í˜„ì¬ ì˜¤ë””ì˜¤ í´ë¦½ ëê¹Œì§€ ë˜ëŠ” ê¸°ë³¸ 3ì´ˆ
            for clip in audio_clips:
                clip_end = clip.start + clip.duration
                if clip.start <= adjusted_start + EPSILON and adjusted_start < clip_end:
                    end_limit = clip_end
                    break
            if end_limit == float('inf'):
                end_limit = adjusted_start + 3.0
        
        duration = max(margin / 2, end_limit - adjusted_start)  # ìµœì†Œ margin/2 (0.25ì´ˆ)
        
        return adjusted_start, duration, clips_to_modify
    
    def _on_image_dropped(self, image_path: str, drop_time: float):
        """Handle image dropped onto timeline via drag and drop (ìŠ¤ë§ˆíŠ¸ ì‚½ì…)"""
        from pathlib import Path
        
        MIN_DURATION = 0.5  # ìµœì†Œ í´ë¦½ ê¸¸ì´ (ì´ˆ)
        
        # ìŠ¤ë§ˆíŠ¸ ìœ„ì¹˜ ê³„ì‚°
        adjusted_start, duration, clips_to_modify = self._calculate_smart_image_position(
            drop_time, MIN_DURATION
        )
        
        # Generate unique ID
        base_id = f"img_dropped_{len([c for c in self.timeline_widget.canvas.clips if c.clip_type == 'image'])}"
        new_id = self._make_unique_clip_id(base_id)
        
        # Create new image clip
        new_clip = TimelineClip(
            id=new_id,
            name=Path(image_path).name,
            start=adjusted_start,
            duration=duration,
            track=2,
            color=QColor("#9E9E9E"),
            clip_type="image",
            waveform=[],
            image_path=image_path
        )
        
        # ê¸°ì¡´ í´ë¦½ ê²½ê³„ ì¡°ì • (ìˆëŠ” ê²½ìš°)
        modifications = []
        for clip, new_end in clips_to_modify:
            old_state = copy.deepcopy(clip)
            clip.duration = new_end - clip.start
            new_state = copy.deepcopy(clip)
            modifications.append((clip.id, old_state, new_state))
        
        # Undo commands (MacroCommandë¡œ ë¬¶ê¸°)
        commands = []
        
        if modifications:
            modify_cmd = ModifyClipsCommand(
                self.timeline_widget.canvas,
                modifications,
                description="ì´ë¯¸ì§€ ì‚½ì…ì„ ìœ„í•œ ê¸°ì¡´ í´ë¦½ ì¡°ì •",
                callback=None  # MacroCommandì—ì„œ ì²˜ë¦¬
            )
            commands.append(modify_cmd)
        add_cmd = AddRemoveClipsCommand(
            self.timeline_widget.canvas,
            added=[new_clip],
            removed=[],
            description=f"ë“œë˜ê·¸ë¡œ ì´ë¯¸ì§€ ì¶”ê°€: {new_clip.name}",
            callback=None # MacroCommand handles callback
        )
        commands.append(add_cmd)
        
        if len(commands) > 1:
            macro_cmd = MacroCommand(
                commands,
                description=f"ìŠ¤ë§ˆíŠ¸ ì´ë¯¸ì§€ ì‚½ì…: {new_clip.name}",
                callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(macro_cmd)
            macro_cmd.redo()
        else:
            # If only one command, make sure it has the callback
            add_cmd.callback = self._on_undo_redo_callback
            self.undo_stack.push(add_cmd)
            add_cmd.redo()
        
        self._update_undo_redo_actions()
        
        self.timeline_widget.canvas._update_total_duration()
        self.timeline_widget.canvas._background_dirty = True
        self.timeline_widget.canvas.update()
        
        # Load image into cache for thumbnail display and prefetch for preview
        from .image_cache import get_image_cache
        cache = get_image_cache()
        if not cache.is_loaded(image_path):
            cache.prefetch_images([image_path])
        
        playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
        
        # ìƒíƒœ ë©”ì‹œì§€
        if adjusted_start != drop_time:
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ê°€ {adjusted_start:.2f}ì´ˆì— ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤: {new_clip.name}")
        else:
            self.statusBar().showMessage(f"ì´ë¯¸ì§€ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: {new_clip.name}")

    
    def _delete_clip(self, clip):
        """Delete a clip from the timeline"""
        if clip in self.timeline_widget.canvas.clips:
            # Undo command
            cmd = AddRemoveClipsCommand(
                self.timeline_widget.canvas,
                added=[],
                removed=[clip],
                description=f"Delete {clip.name}",
                callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(cmd)
            cmd.redo()
            self._update_undo_redo_actions()

            # Remove from AudioMixer if audio clip (handled by _on_undo_redo_callback mostly, but ensure)
            if clip.clip_type == "audio":
                self.preview_widget.remove_audio_clip(clip.id)
            
            self.timeline_widget.canvas._background_dirty = True
            self.timeline_widget.canvas.update()
            playhead_ms = int(self.timeline_widget.canvas.playhead_time * 1000)
            self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips, playhead_ms)
            
            # Update total duration
            all_clips = self.timeline_widget.canvas.clips
            if all_clips:
                total_duration = max((c.start + c.duration for c in all_clips), default=0.0)
                self.preview_widget.set_total_duration(total_duration)
            
            self.statusBar().showMessage(f"í´ë¦½ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {clip.name}")
    
    def _on_clip_delete_requested(self, clip_id: str):
        """Handle delete key press on timeline - delete the selected clip"""
        # Find the clip by ID
        clip = None
        for c in self.timeline_widget.canvas.clips:
            if c.id == clip_id:
                clip = c
                break
        
        if clip:
            self._delete_clip(clip)
            # Clear selection after delete
            self.timeline_widget.canvas.selected_clip = None
    
    def _regenerate_preview_from_clips(self):
        """Rebuild AudioMixer with all current clips"""
        try:
            from .audio_mixer import ScheduledClip
            
            # Use self.speaker_audio_map directly (works for both fresh and loaded projects)
            speaker_audio_map = self.speaker_audio_map or {}
            
            # Get audio clips sorted by start time
            audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
            audio_clips.sort(key=lambda c: c.start)
            
            if not audio_clips:
                return
            
            # Build scheduled clips for AudioMixer
            scheduled_clips = []
            for clip in audio_clips:
                scheduled_clips.append(ScheduledClip(
                    clip_id=clip.id,
                    speaker=clip.speaker,
                    timeline_start=clip.start,
                    timeline_end=clip.start + clip.duration,
                    source_offset=clip.offset,
                    source_path=speaker_audio_map.get(clip.speaker, ""),
                    duration=clip.duration
                ))
            
            # Calculate total timeline duration from ALL clips (audio, subtitle, image)
            all_clips = self.timeline_widget.canvas.clips
            total_timeline_duration = max((c.start + c.duration for c in all_clips), default=0.0)
            
            # Save current playhead position to restore after update
            current_playhead = self.timeline_widget.canvas.playhead_time
            
            # Set up the AudioMixer with new clips
            self.preview_widget.set_audio_clips(scheduled_clips, speaker_audio_map)
            
            # Update preview widget total duration to match timeline
            self.preview_widget.set_total_duration(total_timeline_duration)
            
            # Restore playhead position
            self.preview_widget.audio_mixer.seek(current_playhead)
            
            # Also ensure timeline playhead stays in sync (UI side)
            self.timeline_widget.set_playhead(current_playhead)
            
            self.statusBar().showMessage("ë¯¸ë¦¬ë³´ê¸° ì˜¤ë””ì˜¤ ì—…ë°ì´íŠ¸ë¨")
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.statusBar().showMessage(f"ì˜¤ë””ì˜¤ ì¬ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def _export_srt(self):
        """Export SRT file"""
        # Allow export from loaded projects (timeline clips are the source of truth)
        sub_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "subtitle"]
        if not sub_clips:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ë‚´ë³´ë‚¼ ìë§‰ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        path, _ = QFileDialog.getSaveFileName(
            self, "SRT ì €ì¥", "subtitles.srt", "SRT Files (*.srt)"
        )
        if path:
            try:
                from exporters.srt_generator import SRTGenerator
                
                # Get subtitle clips from timeline
                sub_clips.sort(key=lambda c: c.start)
                
                texts = []
                timestamps = []
                
                for clip in sub_clips:
                    texts.append(clip.name)
                    timestamps.append((clip.start, clip.start + clip.duration))
                
                generator = SRTGenerator()
                entries = generator.generate_entries(texts, timestamps)
                generator.save(entries, path)
                
                QMessageBox.information(self, "ì €ì¥ ì™„ë£Œ", f"SRT íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{path}")
            except Exception as e:
                QMessageBox.critical(self, "ì˜¤ë¥˜", f"SRT ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def _export_xml(self):
        """Export XML file"""
        # Allow export from loaded projects: export current timeline state.
        if not self.timeline_widget.canvas.clips:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ë‚´ë³´ë‚¼ íƒ€ì„ë¼ì¸ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        path, _ = QFileDialog.getSaveFileName(
            self, "XML ì €ì¥", "project.xml", "XML Files (*.xml)"
        )
        if path:
            try:
                from exporters.xml_exporter import XMLExporter, TimelineClip

                speaker_audio_map = (self.result_data.get('speaker_audio_map', {}) if self.result_data else None) or self.speaker_audio_map

                clips: list[TimelineClip] = []
                for clip in self.timeline_widget.canvas.clips:
                    if clip.clip_type == "audio":
                        speaker = clip.speaker
                        if not speaker and ":" in (clip.name or ""):
                            speaker = clip.name.split(":")[0].strip()

                        audio_path = speaker_audio_map.get(speaker, "") if speaker else ""
                        if not audio_path:
                            # Skip clips we can't resolve to a source file
                            continue

                        clips.append(
                            TimelineClip(
                                name=clip.name,
                                file_path=audio_path,
                                start_time=clip.start,
                                end_time=clip.start + clip.duration,
                                track=clip.track,
                                clip_type="audio",
                                source_in=clip.offset,  # Source file offset
                                source_out=clip.offset + clip.duration,  # Source file end
                            )
                        )

                    elif clip.clip_type == "image":
                        if not clip.image_path:
                            continue
                        clips.append(
                            TimelineClip(
                                name=clip.name,
                                file_path=clip.image_path,
                                start_time=clip.start,
                                end_time=clip.start + clip.duration,
                                track=clip.track,
                                clip_type="video",
                            )
                        )

                if not clips:
                    QMessageBox.warning(self, "ì˜¤ë¥˜", "XMLë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆëŠ” í´ë¦½(ì˜¤ë””ì˜¤/ì´ë¯¸ì§€)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    return
                
                exporter = XMLExporter()
                exporter.save(clips, path)
                
                QMessageBox.information(self, "ì €ì¥ ì™„ë£Œ", f"XML íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{path}")
            except Exception as e:
                QMessageBox.critical(self, "ì˜¤ë¥˜", f"XML ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def _render_video(self):
        """Render final video by launching background thread"""
        # Allow rendering from loaded projects (timeline clips are the source of truth)
        audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
        if not audio_clips:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ë Œë”ë§í•  ì˜¤ë””ì˜¤ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Show render settings dialog first
        dialog = RenderSettingsDialog(self, clips=self.timeline_widget.canvas.clips, speaker_audio_map=self.speaker_audio_map)
        if dialog.exec() != QDialog.DialogCode.Accepted:
            return

        render_settings = dialog.get_settings()

        path, _ = QFileDialog.getSaveFileName(
            self, "ì˜ìƒ ì €ì¥", "output.mp4", "Video Files (*.mp4);;Audio Files (*.wav)"
        )
        if not path:
            return
        
        # For .wav, just merge audio synchronously
        if path.endswith('.wav'):
            self._export_audio_only(path)
            return
        
        # For .mp4, launch render thread
        self.action_render.setEnabled(False)
        self.statusBar().showMessage("ë Œë”ë§ ì¤€ë¹„ ì¤‘...")
        
        # Collect clips from timeline
        image_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "image"]
        subtitle_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "subtitle"]
        audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
        
        # Check for missing image files before rendering
        missing_images = []
        for clip in image_clips:
            if clip.image_path and not Path(clip.image_path).exists():
                missing_images.append(f"  - {Path(clip.image_path).name}")
        
        if missing_images:
            unique_missing_images = list(set(missing_images))
            warning_msg = "ì¼ë¶€ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n\n"
            warning_msg += "ğŸ–¼ï¸ ëˆ„ë½ëœ ì´ë¯¸ì§€ íŒŒì¼:\n"
            warning_msg += "\n".join(unique_missing_images[:5])  # Show first 5
            if len(unique_missing_images) > 5:
                warning_msg += f"\n  ... ì™¸ {len(unique_missing_images) - 5}ê°œ"
            warning_msg += "\n\nëˆ„ë½ëœ ì´ë¯¸ì§€ëŠ” ê²€ì€ í™”ë©´ìœ¼ë¡œ ë Œë”ë§ë©ë‹ˆë‹¤.\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
            
            reply = QMessageBox.warning(
                self, "ì´ë¯¸ì§€ íŒŒì¼ ëˆ„ë½ ê²½ê³ ", warning_msg,
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
            )
            if reply != QMessageBox.StandardButton.Yes:
                self.action_render.setEnabled(True)
                self.statusBar().showMessage("ë Œë”ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
        
        # Filter subtitles if disabled in settings
        if not render_settings.get('subtitle_enabled', True):
            subtitle_clips = []

        # Get speaker audio map
        speaker_audio_map = (self.result_data.get('speaker_audio_map', {}) if self.result_data else None) or self.speaker_audio_map

        # Create and start render thread (ALL-IN-ONE FFmpeg)
        self.render_thread = RenderThread(
            image_clips=image_clips,
            audio_clips=audio_clips,
            subtitle_clips=subtitle_clips,
            output_path=path,
            render_settings=render_settings,
            speaker_audio_map=speaker_audio_map
        )
        self.render_thread.progress.connect(self._on_render_progress)
        self.render_thread.finished.connect(self._on_render_finished)
        
        # Create and show progress dialog for rendering
        self.render_progress_dialog = ProgressDialog(self, "ë™ì˜ìƒ ë Œë”ë§")
        self.render_progress_dialog.cancelled.connect(self.render_thread.cancel)
        self.render_thread.progress.connect(self.render_progress_dialog.update_progress)
        self.render_progress_dialog.show()
        
        self.render_thread.start()
    
    def _create_merged_audio(self, output_path: str):
        """Create a merged WAV file from timeline audio clips"""
        from pydub import AudioSegment
        
        speaker_audio_map = (self.result_data.get('speaker_audio_map', {}) if self.result_data else None) or self.speaker_audio_map

        # Load speaker audio files
        speaker_audio: dict[str, AudioSegment] = {}
        for speaker, audio_path in speaker_audio_map.items():
            if audio_path:
                speaker_audio[speaker] = AudioSegment.from_file(audio_path)
        
        # Get audio clips from timeline
        audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
        audio_clips.sort(key=lambda c: c.start)
        
        # Create merged audio
        result_audio = AudioSegment.empty()
        current_pos = 0.0
        
        for clip in audio_clips:
            speaker = clip.speaker
            if speaker in speaker_audio:
                audio = speaker_audio[speaker]
                
                # Add silence gap if needed
                gap_duration = int((clip.start - current_pos) * 1000)
                if gap_duration > 0:
                    result_audio += AudioSegment.silent(duration=gap_duration)
    
                # Calculate original segment duration
                padded_duration_ms = int(clip.duration * 1000)
                segment_duration_ms = padded_duration_ms
                segment_end = clip.offset + (segment_duration_ms / 1000.0)
                
                # Extract segment from source audio
                start_ms = max(0, int(clip.offset * 1000))
                end_ms = min(len(audio), int(segment_end * 1000))
                audio_clip = audio[start_ms:end_ms]
                
                result_audio += audio_clip
                current_pos = clip.start + len(audio_clip) / 1000.0
        
        # Pad with silence to match total timeline duration (e.g. if images extend beyond audio)
        total_timeline_duration = max((c.start + c.duration for c in self.timeline_widget.canvas.clips), default=0.0)
        current_audio_duration = len(result_audio) / 1000.0
        
        if total_timeline_duration > current_audio_duration:
            silence_gap = int((total_timeline_duration - current_audio_duration) * 1000)
            if silence_gap > 0:
                result_audio += AudioSegment.silent(duration=silence_gap)
        
        result_audio.export(output_path, format='wav')

    def _export_audio_only(self, path: str):
        """Export audio-only .wav file"""
        try:
            self._create_merged_audio(path)
            QMessageBox.information(self, "ì €ì¥ ì™„ë£Œ", f"íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{path}")
        except Exception as e:
            QMessageBox.critical(self, "ì˜¤ë¥˜", f"ì˜¤ë””ì˜¤ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
    
    def _on_render_progress(self, progress: int, message: str):
        """Handle render thread progress update"""
        self.statusBar().showMessage(f"{message} ({progress}%)")
    
    def _on_render_finished(self, success: bool, message: str):
        """Handle render thread completion"""
        # Close progress dialog
        if hasattr(self, 'render_progress_dialog') and self.render_progress_dialog:
            self.render_progress_dialog._is_cancelled = True  # Allow closing
            self.render_progress_dialog.close()
            self.render_progress_dialog = None
        
        self.action_render.setEnabled(True)
        if success:
            self.statusBar().showMessage("ë Œë”ë§ ì™„ë£Œ")
            QMessageBox.information(self, "ì™„ë£Œ", message)
        else:
            self.statusBar().showMessage("ë Œë”ë§ ì‹¤íŒ¨" if "ì·¨ì†Œ" not in message else "ë Œë”ë§ ì·¨ì†Œë¨")
            if "ì·¨ì†Œ" not in message:
                QMessageBox.critical(self, "ì˜¤ë¥˜", message)
            
        # Cleanup temp audio
        if hasattr(self, '_temp_render_audio') and self._temp_render_audio:
            import os
            try:
                if os.path.exists(self._temp_render_audio):
                    os.remove(self._temp_render_audio)
            except:
                pass
            self._temp_render_audio = None
    def _export_audio_dialog(self):
        """Show dialog to export audio only"""
        path, _ = QFileDialog.getSaveFileName(
            self, "ì˜¤ë””ì˜¤ ì €ì¥", "output.wav", "Audio Files (*.wav)"
        )
        if path:
            self._export_audio_only(path)

    def _new_project(self):
        """Create a new project"""
        if not self._check_unsaved_changes():
            return
        
        # Clear image cache to prevent stale data
        from .image_cache import get_image_cache
        get_image_cache().clear()
        
        # Reset state
        self.script_path = None
        self.image_folder = None
        self.speakers = []
        self.speaker_audio_map = {}
        self.audio_files = []
        self.result_data = None
        self.project_path = None
        
        # Reset timeline
        self.timeline_widget.set_clips([])
        self.preview_widget.set_timeline_clips([])
        self.undo_stack.clear()
        
        # Reset UI
        self.script_text.setPlainText("")
        self.image_list.clear()
        
        # Reset mapping table
        self.mapping_table.setRowCount(0)
        self.mapping_info.setText("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ë©´ í™”ì ëª©ë¡ì´ í‘œì‹œë©ë‹ˆë‹¤.")
        
        # Reset buttons
        self.action_process.setEnabled(False)
        self.action_format_subs.setEnabled(False)
        self.action_export_srt.setEnabled(False)
        self.action_export_xml.setEnabled(False)
        self.action_render.setEnabled(False)
        self.action_apply_images.setEnabled(False)
        self.action_export_audio.setEnabled(False)
        self.reload_images_action.setEnabled(False)
        
        # Reset preview
        self.preview_widget.clear_preview()
        
        # Reset audio map in canvas
        self.timeline_widget.canvas.speaker_audio_map = {}

        self.mark_clean()
        self.setWindowTitle("PictureBookBuilder")
        self.statusBar().showMessage("ìƒˆ í”„ë¡œì íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _open_project(self):
        """Open an existing project file"""
        if not self._check_unsaved_changes():
            return

        from PyQt6.QtWidgets import QFileDialog
        import json
        
        path, _ = QFileDialog.getOpenFileName(
            self, "í”„ë¡œì íŠ¸ ì—´ê¸°", "",
            "PictureBookBuilder Project (*.pbb);;All Files (*)"
        )
        
        if not path:
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self._load_project_data(data)
            self.undo_stack.clear()
            self.project_path = path
            self.mark_clean()
            self.setWindowTitle(f"PictureBookBuilder - {Path(path).name}")
            self.statusBar().showMessage(f"í”„ë¡œì íŠ¸ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {path}")
            
            # Add to recent projects
            get_recent_projects_manager().add_project(path, Path(path).stem)
            
        except Exception as e:
            from PyQt6.QtWidgets import QMessageBox
            QMessageBox.critical(self, "ì˜¤ë¥˜", f"í”„ë¡œì íŠ¸ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def open_project_file(self, path: str):
        """Open a project file from external caller (e.g., start screen)"""
        import json
        
        if not Path(path).exists():
            QMessageBox.critical(self, "ì˜¤ë¥˜", f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{path}")
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self._load_project_data(data)
            self.undo_stack.clear()
            self.project_path = path
            self.mark_clean()
            self.setWindowTitle(f"PictureBookBuilder - {Path(path).name}")
            self.statusBar().showMessage(f"í”„ë¡œì íŠ¸ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {path}")
            
            # Add to recent projects
            get_recent_projects_manager().add_project(path, Path(path).stem)
            
        except Exception as e:
            QMessageBox.critical(self, "ì˜¤ë¥˜", f"í”„ë¡œì íŠ¸ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def _save_project(self) -> bool:
        """Save current project. Returns True if saved or not needed, False if cancelled/failed."""
        if self.project_path:
            return self._save_to_file(self.project_path)
        else:
            return self._save_project_as()
    
    def _save_project_as(self) -> bool:
        """Save project as a new file. Returns True if saved, False if cancelled/failed."""
        path, _ = QFileDialog.getSaveFileName(
            self, "í”„ë¡œì íŠ¸ ì €ì¥", "", "PictureBookBuilder Files (*.pbb);;All Files (*)"
        )
        if path:
            if not path.endswith('.pbb'):
                path += '.pbb'
            self.project_path = path
            if self._save_to_file(path):
                self.setWindowTitle(f"PictureBookBuilder - {Path(path).name}")
                return True
        return False
            
    def _show_settings(self):
        """Show the settings dialog"""
        dialog = SettingsDialog(self)
        dialog.set_config(self.runtime_config)
        if dialog.exec() == QDialog.DialogCode.Accepted:
            self.mark_modified()

    
    def _check_unsaved_changes(self) -> bool:
        """Check for unsaved changes. Returns True if safe to proceed, False if cancelled."""
        if not self.is_modified():
            return True

        reply = QMessageBox.question(
            self, "ì €ì¥ë˜ì§€ ì•Šì€ ë³€ê²½ì‚¬í•­",
            "í˜„ì¬ í”„ë¡œì íŠ¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
            QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No | QMessageBox.StandardButton.Cancel
        )

        if reply == QMessageBox.StandardButton.Yes:
            return self._save_project()
        elif reply == QMessageBox.StandardButton.No:
            return True
        else: # Cancel
            return False

    def _save_to_file(self, path: str) -> bool:
        """Save project data to file. Returns True on success."""
        import json
        from datetime import datetime
        
        # Serialize clips
        clips_data = []
        for clip in self.timeline_widget.canvas.clips:
            clip_dict = {
                'id': clip.id,
                'name': clip.name,
                'start': clip.start,
                'duration': clip.duration,
                'track': clip.track,
                'color': clip.color.name(),
                'clip_type': clip.clip_type,
                'offset': clip.offset,  # Simplified: single offset field
                'segment_index': clip.segment_index,
                'speaker': clip.speaker,  # Save speaker for audio clips
                'volume': getattr(clip, 'volume', 1.0),
            }
            
            # Add type-specific data
            if clip.clip_type == "image":
                clip_dict['image_path'] = clip.image_path
            
            # Serialize words if available
            if clip.words:
                clip_dict['words'] = [
                    {'text': w.text if hasattr(w, 'text') else str(w),
                     'start': w.start if hasattr(w, 'start') else 0.0,
                     'end': w.end if hasattr(w, 'end') else 0.0}
                    for w in clip.words
                ]
            
            clips_data.append(clip_dict)
        
        # Save script content directly (in case file is moved)
        script_content = self.script_text.toPlainText()
        
        project_data = {
            'version': '1.1',  # Bumped for settings support
            'saved_at': datetime.now().isoformat(),
            'script_path': self.script_path,
            'script_content': script_content,  # Save script content
            'image_folder': self.image_folder,
            'speaker_audio_map': self.speaker_audio_map,
            'clips': clips_data,
            'settings': self.runtime_config.to_dict(),  # Save settings
        }
        
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(project_data, f, ensure_ascii=False, indent=2)
            self.statusBar().showMessage(f"í”„ë¡œì íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {path}")
            self.mark_clean()
            
            # Add to recent projects
            get_recent_projects_manager().add_project(path, Path(path).stem)
            return True
        except Exception as e:
            QMessageBox.critical(self, "ì˜¤ë¥˜", f"í”„ë¡œì íŠ¸ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{str(e)}")
            return False
    
    def _load_project_data(self, data: dict):
        """Load project data from dictionary"""
        from ui.timeline_widget import TimelineClip
        from PyQt6.QtGui import QColor
        
        # Clear image cache before loading new project
        from .image_cache import get_image_cache
        get_image_cache().clear()
        
        # Load basic info
        self.script_path = data.get('script_path')
        self.image_folder = data.get('image_folder')
        self.speaker_audio_map = data.get('speaker_audio_map', {})
        
        # Update canvas map
        self.timeline_widget.canvas.speaker_audio_map = self.speaker_audio_map

        # Load settings if present
        if 'settings' in data:
            self.runtime_config = RuntimeConfig.from_dict(data['settings'])
            set_config(self.runtime_config)
        
        # Load script - prefer from file, fallback to saved content
        if self.script_path and Path(self.script_path).exists():
            with open(self.script_path, 'r', encoding='utf-8') as f:
                self.script_text.setPlainText(f.read())
        elif 'script_content' in data and data['script_content']:
            # Fallback to saved content if file doesn't exist
            self.script_text.setPlainText(data['script_content'])
            print(f"Script file not found: {self.script_path}, using saved content")
        
        # Validate audio files exist
        missing_audio = []
        for speaker, audio_path in self.speaker_audio_map.items():
            if audio_path and not Path(audio_path).exists():
                missing_audio.append(f"  - {speaker}: {Path(audio_path).name}")
        
        # Track missing images
        missing_images = []
        
        # Rebuild clips
        clips = []
        for clip_data in data.get('clips', []):
            # Reconstruct words
            words = []
            if 'words' in clip_data:
                from core.subtitle_processor import WordSegment
                for w in clip_data['words']:
                    words.append(WordSegment(
                        text=w['text'],
                        start=w['start'],
                        end=w['end']
                    ))
            
            # Validate image path exists for image clips
            image_path = clip_data.get('image_path')
            if image_path and not Path(image_path).exists():
                missing_images.append(f"  - {Path(image_path).name}")
            
            clip = TimelineClip(
                id=clip_data['id'],
                name=clip_data['name'],
                start=clip_data['start'],
                duration=clip_data['duration'],
                track=clip_data['track'],
                color=QColor(clip_data['color']),
                clip_type=clip_data['clip_type'],
                waveform=[],  # Will regenerate if needed
                offset=clip_data.get('offset', 0.0),
                segment_index=clip_data.get('segment_index', -1),
                image_path=image_path,
                speaker=clip_data.get('speaker', ''),  # Restore speaker
                words=words,
                volume=clip_data.get('volume', 1.0)
            )
            clips.append(clip)
        
        self.timeline_widget.set_clips(clips)
        self.preview_widget.set_timeline_clips(clips)
        
        # Pre-load all images used in timeline clips (prefetch originals for preview)
        clip_image_paths = [c.image_path for c in clips if c.clip_type == "image" and c.image_path]
        if clip_image_paths:
            from .image_cache import get_image_cache
            get_image_cache().prefetch_images(clip_image_paths)
            print(f"Pre-fetching {len(clip_image_paths)} images from timeline clips")
        
        # Restore speaker-audio mapping table
        self.speakers = list(self.speaker_audio_map.keys())
        self.mapping_table.setRowCount(len(self.speakers))
        for row, speaker in enumerate(self.speakers):
            # Speaker name
            speaker_item = QTableWidgetItem(speaker)
            speaker_item.setFlags(speaker_item.flags() & ~Qt.ItemFlag.ItemIsEditable)
            self.mapping_table.setItem(row, 0, speaker_item)
            
            # Audio file
            audio_path = self.speaker_audio_map.get(speaker, "")
            if audio_path:
                audio_item = QTableWidgetItem(Path(audio_path).name)
                audio_item.setForeground(QColor(100, 200, 100))
            else:
                audio_item = QTableWidgetItem("(í´ë¦­í•˜ì—¬ ì„ íƒ)")
                audio_item.setForeground(QColor(150, 150, 150))
            self.mapping_table.setItem(row, 1, audio_item)
        
        # Update mapping status
        self._update_mapping_status()
        
        # Restore image folder and list with thumbnails
        self.image_list.clear()
        print(f"Loading image folder: {self.image_folder}")
        if self.image_folder and Path(self.image_folder).exists():
            self._populate_image_list(self.image_folder)
            self.reload_images_action.setEnabled(True)
            print(f"  Loaded images with thumbnails")
        else:
            self.reload_images_action.setEnabled(False)
            print(f"  Image folder not found or empty: {self.image_folder}")
        
        # Enable buttons if we have clips
        if clips:
            self.action_format_subs.setEnabled(True)
            self.action_export_srt.setEnabled(True)
            self.action_export_xml.setEnabled(True)
            self.action_render.setEnabled(True)
            self.action_export_audio.setEnabled(True)
            
            # Enable image apply button if we have images
            if self.image_list.count() > 0:
                self.action_apply_images.setEnabled(True)
                self.action_auteur_import.setEnabled(True)
            
            # Load speaker audio cache for waveform regeneration
            self._load_speaker_audio_cache()
            
            # Regenerate waveforms for audio clips
            self._regenerate_waveforms()
            
            # Generate preview audio from clips
            self._regenerate_preview_from_clips()
        
        # Show warning if files are missing
        if missing_audio or missing_images:
            from PyQt6.QtWidgets import QMessageBox
            
            warning_msg = "ì¼ë¶€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n\n"
            
            if missing_audio:
                warning_msg += "ğŸ“€ ëˆ„ë½ëœ ì˜¤ë””ì˜¤ íŒŒì¼:\n"
                warning_msg += "\n".join(missing_audio[:5])  # Show first 5
                if len(missing_audio) > 5:
                    warning_msg += f"\n  ... ì™¸ {len(missing_audio) - 5}ê°œ"
                warning_msg += "\n\n"
            
            if missing_images:
                # Remove duplicates
                unique_missing_images = list(set(missing_images))
                warning_msg += "ğŸ–¼ï¸ ëˆ„ë½ëœ ì´ë¯¸ì§€ íŒŒì¼:\n"
                warning_msg += "\n".join(unique_missing_images[:5])  # Show first 5
                if len(unique_missing_images) > 5:
                    warning_msg += f"\n  ... ì™¸ {len(unique_missing_images) - 5}ê°œ"
            
            warning_msg += "\n\níŒŒì¼ì„ ì›ë˜ ìœ„ì¹˜ë¡œ ë³µì›í•˜ê±°ë‚˜ ë‹¤ì‹œ ì§€ì •í•´ì£¼ì„¸ìš”."
            
            QMessageBox.warning(self, "íŒŒì¼ ëˆ„ë½ ê²½ê³ ", warning_msg)
    
    def _load_speaker_audio_cache(self):
        """Load speaker audio files into cache"""
        from pydub import AudioSegment
        
        self.speaker_audio_cache = {}
        print(f"Loading speaker audio cache. speaker_audio_map: {self.speaker_audio_map}")
        for speaker, audio_path in self.speaker_audio_map.items():
            if audio_path and Path(audio_path).exists():
                try:
                    self.speaker_audio_cache[speaker] = AudioSegment.from_file(audio_path)
                    print(f"  Loaded audio for speaker: {speaker}")
                except Exception as e:
                    print(f"  Failed to load audio for {speaker}: {e}")
            else:
                print(f"  Audio path not found for speaker {speaker}: {audio_path}")
        
        # Pass audio cache to timeline canvas for real-time waveform updates
        self.timeline_widget.canvas.speaker_audio_cache = self.speaker_audio_cache
        self.timeline_widget.canvas.waveform_extractor = self._extract_waveform_from_audio

    
    def _regenerate_waveforms(self):
        """Regenerate waveforms for audio clips from cached audio"""
        regenerated_count = 0
        for clip in self.timeline_widget.canvas.clips:
            if clip.clip_type == "audio":
                # Use clip.speaker attribute directly, fallback to parsing from name
                speaker = clip.speaker if clip.speaker else None
                if not speaker and ":" in clip.name:
                    speaker = clip.name.split(":")[0].strip()
                
                if speaker and speaker in self.speaker_audio_cache:
                    audio = self.speaker_audio_cache[speaker]

                    # Calculate original segment duration
                    padded_duration_ms = int(clip.duration * 1000)
                    segment_duration_ms = padded_duration_ms
                    segment_end = clip.offset + (segment_duration_ms / 1000.0)
                    
                    start_ms = max(0, int(clip.offset * 1000))
                    end_ms = min(len(audio), int(segment_end * 1000))
                    segment = audio[start_ms:end_ms]
                    
                    # Generate waveform
                    clip.waveform = self._extract_waveform_from_audio(segment)
                    regenerated_count += 1
                else:
                    print(f"Could not regenerate waveform for clip: {clip.name}, speaker: {speaker}")
                    print(f"  Available speakers in cache: {list(self.speaker_audio_cache.keys())}")
        
        print(f"Regenerated {regenerated_count} waveforms")
        self.timeline_widget.canvas.update()
    
    def _import_from_auteur(self):
        """Import scene/shot info from Auteur project and auto-place images"""
        from PyQt6.QtWidgets import QFileDialog, QMessageBox
        from PyQt6.QtGui import QColor
        from pathlib import Path
        from core.auteur_importer import process_auteur_import
        from ui.clip import TimelineClip
        from ui.undo_system import AddRemoveClipsCommand
        
        # Check prerequisites
        if not self.image_folder:
            QMessageBox.warning(self, "ê²½ê³ ", "ë¨¼ì € ì´ë¯¸ì§€ í´ë”ë¥¼ ë¶ˆëŸ¬ì™€ì£¼ì„¸ìš”.")
            return
        
        audio_clips = [c for c in self.timeline_widget.canvas.clips if c.clip_type == "audio"]
        if not audio_clips:
            QMessageBox.warning(self, "ê²½ê³ ", "íƒ€ì„ë¼ì¸ì— ì˜¤ë””ì˜¤ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Select Auteur project file
        auteur_file, _ = QFileDialog.getOpenFileName(
            self,
            "Auteur í”„ë¡œì íŠ¸ íŒŒì¼ ì„ íƒ",
            "",
            "JSON íŒŒì¼ (*.json)"
        )
        
        if not auteur_file:
            return
        
        # Calculate timeline end
        all_clips = self.timeline_widget.canvas.clips
        timeline_end = max((c.start + c.duration for c in all_clips), default=0.0)
        
        try:
            # Process import
            placements = process_auteur_import(
                auteur_file=auteur_file,
                image_folder=self.image_folder,
                clips=all_clips,
                timeline_end=timeline_end,
                similarity_threshold=70.0
            )
            
            if not placements:
                QMessageBox.information(
                    self, "ê²°ê³¼", 
                    "ë§¤ì¹­ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
                    "- Auteur í”„ë¡œì íŠ¸ì™€ íƒ€ì„ë¼ì¸ì˜ ëŒ€ì‚¬ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
                    "- ì´ë¯¸ì§€ íŒŒì¼ëª…ì´ n-m í˜•ì‹(ì˜ˆ: 1-1.png)ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
                )
                return
            
            # Check existing image clips
            existing_image_clips = [c for c in all_clips if c.clip_type == "image"]
            if existing_image_clips:
                reply = QMessageBox.warning(
                    self, "ê²½ê³ ",
                    f"ê¸°ì¡´ ì´ë¯¸ì§€ í´ë¦½ {len(existing_image_clips)}ê°œê°€ ëª¨ë‘ ì‚­ì œë©ë‹ˆë‹¤.\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                    QMessageBox.StandardButton.No
                )
                if reply != QMessageBox.StandardButton.Yes:
                    return
            
            # Create new image clips
            new_image_clips = []
            for i, p in enumerate(placements):
                img_clip = TimelineClip(
                    id=self._make_unique_clip_id(f"auteur_img_{p.scene_id}_{p.shot_id}"),
                    name=Path(p.image_path).name,
                    start=p.start_time,
                    duration=p.end_time - p.start_time,
                    track=2,  # Image track
                    color=QColor("#9E9E9E"),
                    clip_type="image",
                    waveform=[],
                    image_path=p.image_path
                )
                new_image_clips.append(img_clip)
            
            # Create undo command
            cmd = AddRemoveClipsCommand(
                self.timeline_widget.canvas,
                added=new_image_clips,
                removed=existing_image_clips,
                description="Import images from Auteur",
                callback=self._on_undo_redo_callback
            )
            self.undo_stack.push(cmd)
            cmd.redo()
            self._update_undo_redo_actions()
            
            # Update timeline
            self.timeline_widget.canvas._update_total_duration()
            self.timeline_widget.canvas._background_dirty = True
            self.timeline_widget.canvas.update()
            
            # Sync to preview
            self.preview_widget.set_timeline_clips(self.timeline_widget.canvas.clips)
            
            # Show result
            self.statusBar().showMessage(
                f"Auteurì—ì„œ ì´ë¯¸ì§€ {len(new_image_clips)}ê°œê°€ ë°°ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            
        except Exception as e:
            QMessageBox.critical(
                self, "ì˜¤ë¥˜", 
                f"Auteur í”„ë¡œì íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨:\n{str(e)}"
            )
            import traceback
            traceback.print_exc()


def main():
    """Application entry point"""
    app = QApplication(sys.argv)
    
    # Apply Modern Dark Theme
    ModernDarkTheme.apply(app)
    
    window = MainWindow()
    window.show()
    
    sys.exit(app.exec())

